{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc1f8d7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-23T22:23:11.513543Z",
     "iopub.status.busy": "2023-05-23T22:23:11.512957Z",
     "iopub.status.idle": "2023-05-23T22:23:13.539048Z",
     "shell.execute_reply": "2023-05-23T22:23:13.537538Z"
    },
    "papermill": {
     "duration": 2.04584,
     "end_time": "2023-05-23T22:23:13.542144",
     "exception": false,
     "start_time": "2023-05-23T22:23:11.496304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\utils\\_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\maskers\\_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\maskers\\_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\maskers\\_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "c:\\Users\\tharu\\anaconda3\\envs\\xai\\lib\\site-packages\\shap\\explainers\\_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## IMPORTS\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import joblib\n",
    "\n",
    "# Importing Shap for shapley values\n",
    "import shap\n",
    "\n",
    "from ordered_set import OrderedSet\n",
    "from scipy.sparse import lil_matrix\n",
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe6fbb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:23:13.616410Z",
     "iopub.status.busy": "2023-05-23T22:23:13.615606Z",
     "iopub.status.idle": "2023-05-23T22:23:16.332440Z",
     "shell.execute_reply": "2023-05-23T22:23:16.330586Z"
    },
    "papermill": {
     "duration": 2.736958,
     "end_time": "2023-05-23T22:23:16.336104",
     "exception": false,
     "start_time": "2023-05-23T22:23:13.599146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset\n",
      "Initializing objects\n",
      "Encoding\n",
      "Dataset created\n",
      "(4999, 11612) (40000, 11612) (5000, 11612) (4999,) (40000,) (5000,)\n",
      "<class 'scipy.sparse._csr.csr_matrix'> <class 'scipy.sparse._csr.csr_matrix'> <class 'scipy.sparse._csr.csr_matrix'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "A collection of pretrained sklearn models.\n",
      "Contains the models ['knn', 'lr', 'rf', 'svm']\n"
     ]
    }
   ],
   "source": [
    "# Import DataSet ds and Models\n",
    "from src.datasets import IMDBDataset\n",
    "\n",
    "ds = IMDBDataset(\n",
    "    config_path=\"./configs/datasets/imdb.yaml\", root=\"datasets/imdb\", download=True\n",
    ")\n",
    "print(\n",
    "    ds.x_test.shape,\n",
    "    ds.x_train.shape,\n",
    "    ds.x_val.shape,\n",
    "    ds.y_test.shape,\n",
    "    ds.y_train.shape,\n",
    "    ds.y_val.shape,\n",
    ")\n",
    "print(\n",
    "    type(ds.x_test),\n",
    "    type(ds.x_train),\n",
    "    type(ds.x_val),\n",
    "    type(ds.y_test),\n",
    "    type(ds.y_train),\n",
    "    type(ds.y_val),\n",
    ")\n",
    "\n",
    "from src.models import AnalysisModels as Models\n",
    "\n",
    "models = Models(\n",
    "    config_path=\"./configs/models/analysis-models.yaml\",\n",
    "    root=\"./models/analysis-models\",\n",
    "    download=True,\n",
    ")\n",
    "print(models)\n",
    "\n",
    "loaded_plain_model_rf = models.rf.model\n",
    "loaded_plain_model_svc = models.svm.model\n",
    "loaded_plain_model_lr = models.lr.model\n",
    "loaded_plain_model_knn = models.knn.model\n",
    "feature_names = ds.feature_names\n",
    "\n",
    "## Preprocess text\n",
    "\n",
    "x_train_imdb = ds.x_train\n",
    "x_test_imdb = ds.x_test\n",
    "x_val_imdb = ds.x_val\n",
    "\n",
    "# Binarize y - Positive is 1\n",
    "y_train_imdb = ds.y_train\n",
    "y_test_imdb = ds.y_test\n",
    "y_val_imdb = ds.y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder = joblib.load(\"datasets/imdb/tfidf.pkl\")\n",
    "loaded_vocab = input_encoder.vocabulary_\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5b5b511",
   "metadata": {
    "papermill": {
     "duration": 0.014519,
     "end_time": "2023-05-23T22:23:57.584948",
     "exception": false,
     "start_time": "2023-05-23T22:23:57.570429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEDC Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47081014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:22.859676Z",
     "iopub.status.busy": "2023-05-23T22:24:22.858557Z",
     "iopub.status.idle": "2023-05-23T22:24:22.865677Z",
     "shell.execute_reply": "2023-05-23T22:24:22.864477Z"
    },
    "papermill": {
     "duration": 0.030868,
     "end_time": "2023-05-23T22:24:22.870299",
     "exception": false,
     "start_time": "2023-05-23T22:24:22.839431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classifier_fn_lr(x, negative_to_positive=0):\n",
    "    \"\"\"Returns the prediction probability of class 1 -> Not class 0\"\"\"\n",
    "    # print('loaded_plain_model_svc.decision_function(x) - ', loaded_plain_model_svc.decision_function(x))\n",
    "    prediction = loaded_plain_model_lr.predict_proba(x)\n",
    "    # If prediction is [1] retrurn the probability of class 1 else return probability of class 0\n",
    "    if negative_to_positive == 1:\n",
    "        return prediction[:, 0]\n",
    "    return prediction[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f737388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not need\n",
    "# get the accuracy score of the model loaded_plain_model_lr\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = loaded_plain_model_lr.predict(x_test_imdb)\n",
    "accuracy = accuracy_score(y_test_imdb, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test_imdb, y_pred)\n",
    "\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92714b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = loaded_plain_model_lr.coef_\n",
    "coefficients = coefficients.reshape(-1)\n",
    "coefficients[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonyms(word, model):\n",
    "    \"\"\" \" Get antonyms of a word and their indices in the feature vector\n",
    "    Args:\n",
    "        word: word to get antonyms for\n",
    "        model: trained model with feature_importances_\n",
    "\n",
    "    Returns:\n",
    "        tuple of antonyms and their indices in the feature vector\n",
    "    \"\"\"\n",
    "    antonyms = []\n",
    "    antonyms_indices = []\n",
    "    feature_importance = []\n",
    "    temp_dict = {}\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            if i.antonyms():\n",
    "                antonyms.append(i.antonyms()[0].name())\n",
    "    # Remove duplicates in antonyms\n",
    "    antonyms = list(set(antonyms))\n",
    "\n",
    "    for word in antonyms:\n",
    "        if word in loaded_vocab:\n",
    "            # antonyms_indices.append(ds.feature_names.tolist().index(word))\n",
    "            # feature_importance.append(\n",
    "            #     abs(coefficients[loaded_vocab[word]]))\n",
    "            temp_dict[word] = abs(coefficients[loaded_vocab[word]])\n",
    "    # Sort the antonyms and their indices based on feature importance\n",
    "    # antonyms_indices = [x for _, x in sorted(\n",
    "    #     zip(feature_importance, antonyms_indices), reverse=True)]\n",
    "    # antonyms = [x for _, x in sorted(\n",
    "    #     zip(feature_importance, antonyms), reverse=True)]\n",
    "    # print(temp_dict)\n",
    "\n",
    "    # return the key with the highest value\n",
    "    if len(temp_dict) > 0:\n",
    "        max_importance_idx = max(temp_dict, key=temp_dict.get)\n",
    "        return [loaded_vocab[max_importance_idx]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    # print(antonyms)\n",
    "    # print(feature_importance)\n",
    "    # if len(feature_importance) > 0:\n",
    "    #     max_importance_idx = np.argmax(feature_importance)\n",
    "    #     return [antonyms_indices[max_importance_idx]]\n",
    "    # else:\n",
    "    #     return []\n",
    "\n",
    "    # if len(antonyms_indices) > 0:\n",
    "    #     return [antonyms_indices[0]]\n",
    "    # else:\n",
    "    #     return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51db9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:14.202042Z",
     "iopub.status.busy": "2023-05-23T22:24:14.201564Z",
     "iopub.status.idle": "2023-05-23T22:24:14.231018Z",
     "shell.execute_reply": "2023-05-23T22:24:14.229850Z"
    },
    "papermill": {
     "duration": 0.050135,
     "end_time": "2023-05-23T22:24:14.234197",
     "exception": false,
     "start_time": "2023-05-23T22:24:14.184062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perturb_fn(x, inst, print_flag=0):\n",
    "    \"\"\"Function to perturb instance x -> Deform the array -> assign 0 to the x-th column\"\"\"\n",
    "    \"\"\"\n",
    "    Returns perturbed instance inst\n",
    "    \"\"\"\n",
    "    inst[:, x] = 0\n",
    "    return inst\n",
    "\n",
    "\n",
    "def replace_fn(x, y, inst, print_flag=0):\n",
    "    \"\"\"Function to perturb instance x -> Deform the array -> assign 0 to the x-th column\"\"\"\n",
    "    \"\"\"\n",
    "    Returns perturbed instance inst\n",
    "    \"\"\"\n",
    "    new_inst = inst.copy()\n",
    "    try:\n",
    "        temp_x = inst[:, x]\n",
    "        temp_y = inst[:, y]\n",
    "        new_inst[:, x] = temp_y\n",
    "        new_inst[:, y] = temp_x\n",
    "    except:\n",
    "        new_inst[:, x] = 0\n",
    "    return new_inst\n",
    "\n",
    "\n",
    "def conditional_replace_fn(x, y, inst, print_flag=0):\n",
    "    for i in range(len(x)):\n",
    "        if isinstance(y[i], str):\n",
    "            inst[:, x[i]] = 0\n",
    "        else:\n",
    "            temp_x = inst[:, x[i]]\n",
    "            temp_y = inst[:, y[i]]\n",
    "            inst[:, x[i]] = temp_y\n",
    "            inst[:, y[i]] = temp_x\n",
    "    return inst\n",
    "\n",
    "\n",
    "def print_instance(pert_inst, ref_inst, feature_names):\n",
    "    \"\"\"Function to print the perturbed instance\"\"\"\n",
    "    \"\"\"\n",
    "    Returns perturbed instance inst\n",
    "    \"\"\"\n",
    "    indices_active_elements_ref = np.nonzero(ref_inst)[1]\n",
    "    indices_active_elements_pert = np.nonzero(pert_inst)[1]\n",
    "    ref_set = set(indices_active_elements_ref)\n",
    "    pert_set = set(indices_active_elements_pert)\n",
    "    # elements in ref_set but not in pert_set\n",
    "    removed_word_indices = ref_set - pert_set\n",
    "    # elements in pert_set but not in ref_set\n",
    "    added_word_indices = pert_set - ref_set\n",
    "    printable_array = []\n",
    "    for item in indices_active_elements_ref:\n",
    "        printable_array.append(\"..\" + feature_names[item] + \"..\")\n",
    "    # Change formatting of removed words\n",
    "    for item in removed_word_indices:\n",
    "        printable_array[printable_array.index(\"..\" + feature_names[item] + \"..\")] = (\n",
    "            \"--\" + feature_names[item] + \"--\"\n",
    "        )\n",
    "    # change formatting of added words\n",
    "    for item in added_word_indices:\n",
    "        printable_array.append(\"++\" + feature_names[item] + \"++\")\n",
    "    printable_array.append(classifier_fn_lr(pert_inst))\n",
    "    print(printable_array)\n",
    "    return printable_array\n",
    "\n",
    "\n",
    "def print_ref_instance(ref_inst, feaaure_names):\n",
    "    printable_array = []\n",
    "    indices_active_elements = np.nonzero(ref_inst)[1]\n",
    "    for item in indices_active_elements:\n",
    "        printable_array.append(\"..\" + feature_names[item] + \"..\")\n",
    "    print(printable_array)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    - comb: \"best-first\" (combination of) feature(s) that is expanded\n",
    "    (e.g., comb_to_expand)\n",
    "    - expanded_combis: list of combinations of features that are already \n",
    "    expanded as \"best-first\"\n",
    "    - feature_set: indices of the active features of the instance \n",
    "    - candidates_to_expand: combinations of features that are candidates to be \n",
    "    expanded in next iterations or candidates for \"best-first\"\n",
    "    - explanations_sets: counterfactual explanations already found\n",
    "    - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "    combinations of features to be expanded\n",
    "    - instance: instance to be explained\n",
    "    - cf: classifier prediction probability function\n",
    "    or decision function. For ScikitClassifiers, this is classifier.predict_proba \n",
    "    or classifier.decision_function or classifier.predict_log_proba.\n",
    "    Make sure the function only returns one (float) value. For instance, if you\n",
    "    use a ScikitClassifier, transform the classifier.predict_proba as follows:\n",
    "\n",
    "        def classifier_fn(X):\n",
    "            c=classification_model.predict_proba(X)\n",
    "            y_predicted_proba=c[:,1]\n",
    "            return y_predicted_proba\n",
    "\n",
    "Returns:\n",
    "    - explanation_candidates: combinations of features that are explanation\n",
    "    candidates to be checked in the next iteration\n",
    "    - candidates_to_expand: combinations of features that are candidates to be \n",
    "    expanded in next iterations or candidates for \"best-first\"\n",
    "    - expanded_combis: [list] list of combinations of features that are already \n",
    "    expanded as \"best-first\" \n",
    "    - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "    combinations of features to be expanded\n",
    "    - scores_explanation_candidates: scores after perturbation of explanation candidates\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def expand_and_prune(\n",
    "    comb,\n",
    "    replacement_comb_to_expand,\n",
    "    expanded_combis,\n",
    "    feature_set,\n",
    "    candidates_to_expand,\n",
    "    candidates_to_expand_replacements,\n",
    "    explanations_sets,\n",
    "    explanation_replacement_sets,\n",
    "    scores_candidates_to_expand,\n",
    "    instance,\n",
    "    cf,\n",
    "    revert=0,\n",
    "    replacements=[],\n",
    "):\n",
    "    \"\"\"Function to expand \"best-first\" feature combination and prune explanation_candidates and candidates_to_expand\"\"\"\n",
    "\n",
    "    comb = OrderedSet(comb)\n",
    "    replacement_comb_to_expand = OrderedSet(replacement_comb_to_expand)\n",
    "    print(\"comb: \", comb)\n",
    "    print(\"replacement_comb_to_expand: \", replacement_comb_to_expand)\n",
    "    expanded_combis.append(comb)\n",
    "\n",
    "    old_candidates_to_expand = [frozenset(x) for x in candidates_to_expand]\n",
    "    old_candidates_to_expand = set(old_candidates_to_expand)\n",
    "    print(\"feature_set: \", feature_set)\n",
    "    feature_set_new = []\n",
    "    feature_set_new_replacements = []\n",
    "    ## If the feature is not in the current combination -> add it to a new list\n",
    "    for feature in feature_set:\n",
    "        list_feature = list(feature)\n",
    "        if len(comb & feature) == 0:  # set operation: intersection\n",
    "            replacement_feature = get_antonyms(\n",
    "                feature_names[list_feature[0]], loaded_plain_model_rf\n",
    "            )\n",
    "            replacement_feature = frozenset(replacement_feature)\n",
    "            if replacement_feature == frozenset():\n",
    "                new_string = \"0\" * (len(comb) + 1)\n",
    "                replacement_feature = frozenset([new_string])\n",
    "            # print(\"replacement_feature: \", replacement_feature, \"feature: \", feature)\n",
    "            feature_set_new.append(\n",
    "                feature\n",
    "            )  # If the feature is not in the current combination to remove from the instance\n",
    "            feature_set_new_replacements.append(replacement_feature)\n",
    "\n",
    "    print(\"feature_set_new: \", feature_set_new)\n",
    "    print(\"feature_set_new_replacements: \", feature_set_new_replacements)\n",
    "    # Add each element in the new set -> which were initially not present -> to the accepted combination -> create new combinations -> (EXPANSION)\n",
    "    new_explanation_candidates = []\n",
    "    new_explanation_candidates_replacements = []\n",
    "    # for element in feature_set_new:\n",
    "    #     union = (comb|element) #set operation: union\n",
    "    #     union_replacements = (replacement_comb_to_expand|feature_set_new_replacements[i])\n",
    "    #     new_explanation_candidates.append(union) # Create new combinations to remove from the instance\n",
    "    #     new_explanation_candidates_replacements.append(union_replacements)\n",
    "\n",
    "    for i in range(len(feature_set_new)):\n",
    "        union = comb | feature_set_new[i]\n",
    "        union_replacements = (\n",
    "            replacement_comb_to_expand | feature_set_new_replacements[i]\n",
    "        )\n",
    "        new_explanation_candidates.append(\n",
    "            union\n",
    "        )  # Create new combinations to remove from the instance\n",
    "        new_explanation_candidates_replacements.append(union_replacements)\n",
    "\n",
    "    print(\"new_explanation_candidates: \", new_explanation_candidates)\n",
    "    print(\n",
    "        \"new_explanation_candidates_replacements: \",\n",
    "        new_explanation_candidates_replacements,\n",
    "    )\n",
    "\n",
    "    # Add new explanation candidates to the list of candidates to expand\n",
    "    candidates_to_expand_notpruned = candidates_to_expand.copy()\n",
    "    candidates_to_expand_replacements_notpruned = (\n",
    "        candidates_to_expand_replacements.copy()\n",
    "    )\n",
    "    # for new_candidate in new_explanation_candidates:\n",
    "    #     candidates_to_expand_notpruned.append(new_candidate)\n",
    "    for i in range(len(new_explanation_candidates_replacements)):\n",
    "        candidates_to_expand_notpruned.append(new_explanation_candidates[i])\n",
    "        candidates_to_expand_replacements_notpruned.append(\n",
    "            new_explanation_candidates_replacements[i]\n",
    "        )\n",
    "\n",
    "    print(\"candidates_to_expand_notpruned: \", candidates_to_expand_notpruned)\n",
    "    print(\n",
    "        \"candidates_to_expand_replacements_notpruned: \",\n",
    "        candidates_to_expand_replacements_notpruned,\n",
    "    )\n",
    "\n",
    "    # Calculate scores of new combinations and add to scores_candidates_to_expand\n",
    "    # perturb each new candidate and get the score for each.\n",
    "    # perturbed_instances = [perturb_fn(x, inst=instance.copy(), print_flag=1) for x in new_explanation_candidates]\n",
    "    replaced_instances = []\n",
    "    for i in range(len(new_explanation_candidates)):\n",
    "        #     #print(\"i: \", i, \"new_explanation_candidates[i]: \", new_explanation_candidates[i], \"new_explanation_candidates_replacements[i]: \", new_explanation_candidates_replacements[i])\n",
    "        #     if isinstance(new_explanation_candidates_replacements[i][0], int):\n",
    "        #         replaced_instances.append(perturb_fn(x=new_explanation_candidates[i], inst=instance.copy()))\n",
    "        #     else:\n",
    "        replaced_instances.append(\n",
    "            conditional_replace_fn(\n",
    "                x=new_explanation_candidates[i],\n",
    "                y=new_explanation_candidates_replacements[i],\n",
    "                inst=instance.copy(),\n",
    "                print_flag=1,\n",
    "            )\n",
    "        )\n",
    "    # -------------------------------------------\n",
    "    print(\"len(perturbed_instances): \", len(replaced_instances))\n",
    "    perturbed_instances = replaced_instances\n",
    "\n",
    "    # word_sets = []\n",
    "    # replacement_word_sets = []\n",
    "    # for item in new_explanation_candidates:\n",
    "    #     item_word = []\n",
    "    #     word_replacement = []\n",
    "    #     for index in item:\n",
    "    #         item_word.append(feature_names[index])\n",
    "    #         word_replacement.append(get_antonyms(feature_names[index], loaded_plain_model_rf))\n",
    "    #     word_sets.append(item_word)\n",
    "    #     replacement_word_sets.append(word_replacement)\n",
    "    # print(word_sets)\n",
    "    # print(replacement_word_sets)\n",
    "    # #replacements = np.array(replacement_word_sets).reshape(len(replacement_word_sets), 1)\n",
    "    # replacements = []\n",
    "    # # for features in replacement_word_sets:\n",
    "    # #     replacements.append(OrderedSet(features))\n",
    "    # # replacements = [frozenset(x) for x in replacements]\n",
    "\n",
    "    # replaced_instances = []\n",
    "    # perturbed_instances = [perturb_fn(x, inst=instance.copy()) for x in new_explanation_candidates]\n",
    "    print(\"Expanded sentences from the above chosen combination\")\n",
    "    for item in perturbed_instances:\n",
    "        print_instance(item, instance.copy(), feature_names)\n",
    "    scores_perturbed_new = [cf(x, revert) for x in perturbed_instances]\n",
    "    ## Append the newly created score array to the passes existing array\n",
    "    scores_candidates_to_expand_notpruned = (\n",
    "        scores_candidates_to_expand + scores_perturbed_new\n",
    "    )\n",
    "    # create a dictionary of scores dictionary where the\n",
    "    # keys are string representations of the candidates from candidates_to_expand_notpruned, and the\n",
    "    # values are the corresponding scores from scores_candidates_to_expand_notpruned\n",
    "    dictionary_scores = dict(\n",
    "        zip(\n",
    "            [str(x) for x in candidates_to_expand_notpruned],\n",
    "            scores_candidates_to_expand_notpruned,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # *** Pruning step: remove all candidates to expand that have an explanation as subset ***\n",
    "    candidates_to_expand_pruned_explanations = []\n",
    "    candidates_to_expand_pruned_replacements_explanations = []\n",
    "    # take one combination from candidates\n",
    "    # Rewritten using list indices\n",
    "    # for combi in candidates_to_expand_notpruned:\n",
    "    #     pruning=0\n",
    "    #     for explanation in explanations_sets: # if an explanation is present as a subser in combi, does not add it to the to be expanded list -> because solution with a smaller size exists\n",
    "    #         if ((explanation.issubset(combi)) or (explanation==combi)):\n",
    "    #             pruning = pruning + 1\n",
    "    #     if (pruning == 0): # If it is not a superset of a present explanation -> add it to the list\n",
    "    #         candidates_to_expand_pruned_explanations.append(combi)\n",
    "    # Write the above function using list indices\n",
    "    for i in range(len(candidates_to_expand_notpruned)):\n",
    "        pruning = 0\n",
    "        for explanation in explanations_sets:\n",
    "            if (explanation.issubset(candidates_to_expand_notpruned[i])) or (\n",
    "                explanation == candidates_to_expand_notpruned[i]\n",
    "            ):\n",
    "                pruning = pruning + 1\n",
    "        if pruning == 0:\n",
    "            candidates_to_expand_pruned_explanations.append(\n",
    "                candidates_to_expand_notpruned[i]\n",
    "            )\n",
    "            candidates_to_expand_pruned_replacements_explanations.append(\n",
    "                candidates_to_expand_replacements_notpruned[i]\n",
    "            )\n",
    "\n",
    "    # Each element is frozen as a set\n",
    "    candidates_to_expand_pruned_explanations_frozen = [\n",
    "        frozenset(x) for x in candidates_to_expand_pruned_explanations\n",
    "    ]\n",
    "    candidates_to_expand_pruned_replacements_explanations_frozen = [\n",
    "        frozenset(x) for x in candidates_to_expand_pruned_replacements_explanations\n",
    "    ]\n",
    "\n",
    "    # But the total set f frozen sets are not frozen\n",
    "    candidates_to_expand_pruned_explanations_ = set(\n",
    "        candidates_to_expand_pruned_explanations_frozen\n",
    "    )\n",
    "    candidates_to_expand_pruned_replacements_explanations_ = set(\n",
    "        candidates_to_expand_pruned_replacements_explanations_frozen\n",
    "    )\n",
    "\n",
    "    expanded_combis_frozen = [frozenset(x) for x in expanded_combis]\n",
    "    expanded_combis_ = set(expanded_combis_frozen)\n",
    "\n",
    "    # *** Pruning step: remove all candidates to expand that are in expanded_combis *** -> Same as above\n",
    "    candidates_to_expand_pruned = (\n",
    "        candidates_to_expand_pruned_explanations_ - expanded_combis_\n",
    "    )\n",
    "    candidates_to_expand_pruned_replacements = (\n",
    "        candidates_to_expand_pruned_replacements_explanations_ - expanded_combis_\n",
    "    )\n",
    "    ind_dict = dict(\n",
    "        (k, i) for i, k in enumerate(candidates_to_expand_pruned_explanations_frozen)\n",
    "    )\n",
    "    indices = [ind_dict[x] for x in candidates_to_expand_pruned]\n",
    "    candidates_to_expand = [\n",
    "        candidates_to_expand_pruned_explanations[i] for i in indices\n",
    "    ]\n",
    "    candidates_to_expand_replacements = [\n",
    "        candidates_to_expand_pruned_replacements_explanations[i] for i in indices\n",
    "    ]\n",
    "\n",
    "    # The new explanation candidates are the ones that are NOT in the old list of candidates to expand\n",
    "    new_explanation_candidates_pruned = (\n",
    "        candidates_to_expand_pruned - old_candidates_to_expand\n",
    "    )\n",
    "    candidates_to_expand_frozen = [frozenset(x) for x in candidates_to_expand]\n",
    "    candidates_to_expand_replacements_frozen = [\n",
    "        frozenset(x) for x in candidates_to_expand_replacements\n",
    "    ]\n",
    "\n",
    "    ind_dict2 = dict((k, i) for i, k in enumerate(candidates_to_expand_frozen))\n",
    "    indices2 = [ind_dict2[x] for x in new_explanation_candidates_pruned]\n",
    "    explanation_candidates = [candidates_to_expand[i] for i in indices2]\n",
    "    explanation_candidates_replacements = [\n",
    "        candidates_to_expand_replacements[i] for i in indices2\n",
    "    ]\n",
    "\n",
    "    # Get scores of the new candidates and explanations.\n",
    "    scores_candidates_to_expand = [\n",
    "        dictionary_scores[x] for x in [str(c) for c in candidates_to_expand]\n",
    "    ]\n",
    "    scores_explanation_candidates = [\n",
    "        dictionary_scores[x] for x in [str(c) for c in explanation_candidates]\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        explanation_candidates,\n",
    "        explanation_candidates_replacements,\n",
    "        candidates_to_expand,\n",
    "        candidates_to_expand_replacements,\n",
    "        expanded_combis,\n",
    "        scores_candidates_to_expand,\n",
    "        scores_explanation_candidates,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEDC_Explainer(object):\n",
    "    \"\"\"Class for generating evidence counterfactuals for classifiers on behavioral/text data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_names,\n",
    "        classifier_fn,\n",
    "        threshold_classifier,\n",
    "        max_iter=100,\n",
    "        max_explained=1,\n",
    "        BB=True,\n",
    "        max_features=30,\n",
    "        time_maximum=120,\n",
    "        revert=0,\n",
    "    ):\n",
    "        \"\"\"Init function\n",
    "\n",
    "        Args:\n",
    "            classifier_fn: [function] classifier prediction probability function\n",
    "            or decision function. For ScikitClassifiers, this is classifier.predict_proba\n",
    "            or classifier.decision_function or classifier.predict_log_proba.\n",
    "            Make sure the function only returns one (float) value. For instance, if you\n",
    "            use a ScikitClassifier, transform the classifier.predict_proba as follows:\n",
    "\n",
    "                def classifier_fn(X):\n",
    "                    c=classification_model.predict_proba(X)\n",
    "                    y_predicted_proba=c[:,1]\n",
    "                    return y_predicted_proba\n",
    "\n",
    "            threshold_classifier: [float] the threshold that is used for classifying\n",
    "            instances as positive or not. When score or probability exceeds the\n",
    "            threshold value, then the instance is predicted as positive.\n",
    "            We have no default value, because it is important the user decides\n",
    "            a good value for the threshold.\n",
    "\n",
    "            feature_names: [numpy.array] contains the interpretable feature names,\n",
    "            such as the words themselves in case of document classification or the names\n",
    "            of visited URLs.\n",
    "\n",
    "            max_iter: [int] maximum number of iterations in the search procedure.\n",
    "            Default is set to 50.\n",
    "\n",
    "            max_explained: [int] maximum number of EDC explanations generated.\n",
    "            Default is set to 1.\n",
    "\n",
    "            BB: [“True” or “False”]  when the algorithm is augmented with\n",
    "            branch-and-bound (BB=True), one is only interested in the (set of)\n",
    "            shortest explanation(s). Default is \"True\".\n",
    "\n",
    "            max_features: [int] maximum number of features allowed in the explanation(s).\n",
    "            Default is set to 30.\n",
    "\n",
    "            time_maximum: [int] maximum time allowed to generate explanations,\n",
    "            expressed in minutes. Default is set to 2 minutes (120 seconds).\n",
    "        \"\"\"\n",
    "\n",
    "        self.feature_names = feature_names\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.threshold_classifier = threshold_classifier\n",
    "        self.max_iter = max_iter\n",
    "        self.max_explained = max_explained\n",
    "        self.BB = BB\n",
    "        self.max_features = max_features\n",
    "        self.time_maximum = time_maximum\n",
    "        self.revert = None\n",
    "        self.initial_class = None\n",
    "        self._report_data = {}\n",
    "\n",
    "    def explanation(self, instance):\n",
    "        \"\"\"Generates evidence counterfactual explanation for the instance.\n",
    "        ONLY IF THE CURRENT INSTANCE IS POSITIVE -> Limitation\n",
    "\n",
    "        Args:\n",
    "            instance: [numpy.array or sparse matrix] instance to explain\n",
    "\n",
    "        Returns:\n",
    "            A dictionary where:\n",
    "\n",
    "                explanation_set: explanation(s) ranked from high to low change\n",
    "                in predicted score or probability.\n",
    "                The number of explanations shown depends on the argument max_explained.\n",
    "\n",
    "                number_active_elements: number of active elements of\n",
    "                the instance of interest.\n",
    "\n",
    "                number_explanations: number of explanations found by algorithm.\n",
    "\n",
    "                minimum_size_explanation: number of features in the smallest explanation.\n",
    "\n",
    "                time_elapsed: number of seconds passed to generate explanation(s).\n",
    "\n",
    "                explanations_score_change: change in predicted score/probability\n",
    "                when removing the features in the explanation, ranked from\n",
    "                high to low change.\n",
    "        \"\"\"\n",
    "\n",
    "        # *** INITIALIZATION ***\n",
    "        print(\"Start initialization...\")\n",
    "        tic = time.time()\n",
    "        instance = lil_matrix(instance)\n",
    "        print(\"initial sentence is ... \")\n",
    "        print(instance.get_shape())\n",
    "        print_ref_instance(instance, self.feature_names)\n",
    "        iteration = 0\n",
    "        nb_explanations = 0\n",
    "        minimum_size_explanation = np.nan\n",
    "        explanations = []\n",
    "        explanations_replacements = []\n",
    "        explanations_sets = []\n",
    "        explanation_replacement_sets = []\n",
    "        explanations_score_change = []\n",
    "        expanded_combis = []\n",
    "        score_predicted = self.classifier_fn(instance)  ## Returns Prediction Prob\n",
    "        # Intial class is 1 is score is greater than threshold\n",
    "        if score_predicted > self.threshold_classifier:\n",
    "            self.initial_class = [1]\n",
    "        else:\n",
    "            self.initial_class = [0]\n",
    "            self.revert = 1\n",
    "        print(\n",
    "            \"score_predicted  \",\n",
    "            score_predicted,\n",
    "            \"  initial_class  \",\n",
    "            self.initial_class,\n",
    "        )\n",
    "\n",
    "        reference = np.reshape(\n",
    "            np.zeros(np.shape(instance)[1]), (1, len(np.zeros(np.shape(instance)[1])))\n",
    "        )\n",
    "        reference = sparse.csr_matrix(reference)\n",
    "\n",
    "        # explainer = shap.KernelExplainer(self.classifier_fn, reference, link=\"identity\")\n",
    "        # shapVals = explainer.shap_values(instance, nsamples=5000, l1_reg=\"aic\")\n",
    "\n",
    "        # features = []\n",
    "        # for ind in range(len(shapVals[0])):\n",
    "        #     if shapVals[0, ind] != 0:\n",
    "        #         features.append({\"feature\": ind, \"shapValue\": shapVals[0, ind]})\n",
    "        # sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"], reverse=True)\n",
    "        # inverse_sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"])\n",
    "\n",
    "        # if self.revert == 1:\n",
    "        #     sorted_data_in = inverse_sorted_data_in\n",
    "\n",
    "        indices_active_elements = np.nonzero(instance)[\n",
    "            1\n",
    "        ]  ## -> Gets non zero elements in the instance as an array [x, y, z]\n",
    "        # sorted_indices = sorted(\n",
    "        #     indices_active_elements, key=lambda x: shapVals[0, x], reverse=True\n",
    "        # )\n",
    "        # indices_active_elements = np.array(sorted_indices)\n",
    "        number_active_elements = len(indices_active_elements)\n",
    "        indices_active_elements = indices_active_elements.reshape(\n",
    "            (number_active_elements, 1)\n",
    "        )  ## -> Reshape to get a predictable\n",
    "\n",
    "        candidates_to_expand = (\n",
    "            []\n",
    "        )  # -> These combinations are further expanded -> These are the elements to be removed from the sentence\n",
    "        for features in indices_active_elements:\n",
    "            candidates_to_expand.append(OrderedSet(features))\n",
    "        ## > Gets an array with each element in reshaped incides as an ordered set -> [OrderedSet([430]), OrderedSet([588]), OrderedSet([595])]\n",
    "        candidates_to_expand_replacements = []\n",
    "        for features in indices_active_elements:\n",
    "            candidates_to_expand_replacements.append(\n",
    "                OrderedSet(\n",
    "                    get_antonyms(self.feature_names[features[0]], loaded_plain_model_rf)\n",
    "                )\n",
    "            )\n",
    "        for i in range(len(candidates_to_expand_replacements)):\n",
    "            if candidates_to_expand_replacements[i] == OrderedSet():\n",
    "                candidates_to_expand_replacements[i] = OrderedSet([\"0\"])\n",
    "        explanation_candidates = candidates_to_expand.copy()\n",
    "        explanation_candidates_replacements = candidates_to_expand_replacements.copy()\n",
    "        print(\"explanation_candidates /n\", explanation_candidates)\n",
    "        print(\n",
    "            \"explanation_candidates_replacements /n\",\n",
    "            explanation_candidates_replacements,\n",
    "        )\n",
    "        ## Gets a copy of the above array -> Initially\n",
    "\n",
    "        feature_set = [\n",
    "            frozenset(x) for x in indices_active_elements\n",
    "        ]  ## Immutable -> can be used as keys in dictionary\n",
    "        ## Used features in the current x-reference -> incides of the words in the review.\n",
    "\n",
    "        print(\"Initialization is complete.\")\n",
    "        print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** WHILE LOOP ***\n",
    "        while (\n",
    "            (iteration < self.max_iter)\n",
    "            and (nb_explanations < self.max_explained)\n",
    "            and (len(candidates_to_expand) != 0)\n",
    "            and (len(explanation_candidates) != 0)\n",
    "            and ((time.time() - tic) < self.time_maximum)\n",
    "        ):\n",
    "            ## Stop if maximum iterations exceeded\n",
    "            #  number of explanations generated is greater than the maximum explanations\n",
    "            #  There are no candidates to expand\n",
    "            #  There are no explanation candidates -> Used to force stop while loop below\n",
    "            #  Or maximum allowed time exceeded\n",
    "            iteration += 1\n",
    "            print(\"\\n Iteration %d \\n\" % iteration)\n",
    "\n",
    "            if iteration == 1:\n",
    "                print(\"Run in first iteration -> perturbation done \\n\")\n",
    "                # Print the word in each index in the explanation candidates\n",
    "                # for item in explanation_candidates:\n",
    "                #     print([self.feature_names[x] for x in item])\n",
    "                replacements = [\n",
    "                    get_antonyms(self.feature_names[x[0]], loaded_plain_model_rf)\n",
    "                    for x in explanation_candidates\n",
    "                ]\n",
    "                # convert each element in replacement to a OrderedSet\n",
    "                replacements = explanation_candidates_replacements\n",
    "                print(\"replacements \\n\", replacements, \"\\n\")\n",
    "                print(\"explanation_candidates \\n\", explanation_candidates, \"\\n\")\n",
    "                perturbed_instances = []\n",
    "                print(\"After changing or removing words,  \")\n",
    "                replaced_instances = []\n",
    "                for i in range(len(explanation_candidates)):\n",
    "                    if replacements[i] == OrderedSet([\"0\"]):\n",
    "                        replaced_instances.append(\n",
    "                            perturb_fn(\n",
    "                                x=explanation_candidates[i], inst=instance.copy()\n",
    "                            )\n",
    "                        )\n",
    "                    else:\n",
    "                        replaced_instances.append(\n",
    "                            replace_fn(\n",
    "                                x=explanation_candidates[i],\n",
    "                                y=replacements[i],\n",
    "                                inst=instance.copy(),\n",
    "                            )\n",
    "                        )\n",
    "                # Remove the elements in the indices given by the ordered set x and return an array fo such elements\n",
    "                # Removes only one element in the first run -> Contains sentences with one word removed\n",
    "                perturbed_instances = replaced_instances\n",
    "                for instance_p in perturbed_instances:\n",
    "                    print_instance(instance_p, instance, self.feature_names)\n",
    "                scores_explanation_candidates = [\n",
    "                    self.classifier_fn(x, self.revert) for x in perturbed_instances\n",
    "                ]\n",
    "                # Get predictions for each perturbed instance where one or more elements are removed from the initial instance\n",
    "                # It is in form of [[x], [y], [z]]\n",
    "                print(\n",
    "                    \"scores_explanation_candidates \\n\",\n",
    "                    scores_explanation_candidates,\n",
    "                    \"\\n\",\n",
    "                )\n",
    "                scores_candidates_to_expand = scores_explanation_candidates.copy()\n",
    "\n",
    "            scores_perturbed_new_combinations = [\n",
    "                x[0] for x in scores_explanation_candidates\n",
    "            ]\n",
    "            # Therefore get it to the shape [x, y, z] by getting the [0] th element of each element array\n",
    "            print(\n",
    "                \"scores_perturbed_new_combinations \", scores_perturbed_new_combinations\n",
    "            )\n",
    "\n",
    "            # ***CHECK IF THERE ARE EXPLANATIONS***\n",
    "            new_explanations = list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            new_explanation_replacements = list(\n",
    "                compress(\n",
    "                    explanation_candidates_replacements,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # Get explanation candidates where their probability is less than the threshold classifier -> Positive becomes negative\n",
    "            # print(\"New Explanations \\n\", new_explanations)\n",
    "            explanations += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            explanations_replacements += list(\n",
    "                compress(\n",
    "                    explanation_candidates_replacements,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # print(\"\\n explanations, explanations_score_change\", explanations)\n",
    "            nb_explanations += len(\n",
    "                list(\n",
    "                    compress(\n",
    "                        explanation_candidates,\n",
    "                        scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                    )\n",
    "                )\n",
    "            )  # Update number of explanations which pass the required threshold\n",
    "            explanations_sets += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            explanation_replacement_sets += list(\n",
    "                compress(\n",
    "                    explanation_candidates_replacements,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            explanations_sets = [\n",
    "                set(x) for x in explanations_sets\n",
    "            ]  # Convert each array to a set -> to get the words\n",
    "            explanation_replacement_sets = [\n",
    "                set(x) for x in explanation_replacement_sets\n",
    "            ]\n",
    "            explanations_score_change += list(\n",
    "                compress(\n",
    "                    scores_explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            print(\"explanations_score_change\", explanations_score_change)\n",
    "\n",
    "            # Adjust max_length\n",
    "            if self.BB == True:\n",
    "                if len(explanations) != 0:\n",
    "                    lengths = []  # Record length of each explanation found\n",
    "                    for explanation in explanations:\n",
    "                        lengths.append(len(explanation))\n",
    "                    lengths = np.array(lengths)\n",
    "                    max_length = lengths.min()\n",
    "                    # Get minimum length of the found explanations as max length -> Do not search for explanations with longer length\n",
    "                else:\n",
    "                    max_length = number_active_elements  # Else can find maximum length equal to number of words in instance\n",
    "            else:\n",
    "                max_length = number_active_elements\n",
    "            print(\"\\n-------------Max length updated to - \", max_length)\n",
    "\n",
    "            # Eliminate combinations from candidates_to_expand (\"best-first\" candidates) that can not be expanded\n",
    "            # Pruning based on Branch & Bound=True, max. features allowed and number of active features\n",
    "            candidates_to_expand_updated = []\n",
    "            candidates_to_expand_updated_replacements = []\n",
    "            scores_candidates_to_expand_updated = (\n",
    "                []\n",
    "            )  # enumerate -> Find count of || to list one after another\n",
    "            for j, combination in enumerate(candidates_to_expand):\n",
    "                if (\n",
    "                    (len(combination) < number_active_elements)\n",
    "                    and (len(combination) < max_length)\n",
    "                    and (len(combination) < self.max_features)\n",
    "                ):\n",
    "                    # Combination length should be less than the words in the input and max length of the required explanation and required maximum features\n",
    "                    candidates_to_expand_updated.append(\n",
    "                        combination\n",
    "                    )  # If the combination matches, it is further expanded\n",
    "                    scores_candidates_to_expand_updated.append(\n",
    "                        scores_candidates_to_expand[j]\n",
    "                    )\n",
    "                    # Add the prediction score to the new array\n",
    "                    # get the score from the scores_candidates_to_expand using the current index\n",
    "                    candidates_to_expand_updated_replacements.append(\n",
    "                        candidates_to_expand_replacements[j]\n",
    "                    )\n",
    "                    # Add the replacement to the new array\n",
    "\n",
    "            print(\n",
    "                \"\\nlen(candidates_to_expand_updated)\",\n",
    "                len(candidates_to_expand_updated),\n",
    "                \" 0 \",\n",
    "            )\n",
    "            print(\n",
    "                \"\\nnb_explanations\",\n",
    "                nb_explanations,\n",
    "                \" >= self.max_explained \",\n",
    "                self.max_explained,\n",
    "            )\n",
    "\n",
    "            # *** IF LOOP ***\n",
    "            # expanding the candidates to update will exceed the max length set in the earlier loop\n",
    "            if (len(candidates_to_expand_updated) == 0) or (\n",
    "                nb_explanations >= self.max_explained\n",
    "            ):\n",
    "                ## If the number of explanations exceeded the required number\n",
    "                ## or no candidates\n",
    "                ## no explanations present\n",
    "\n",
    "                print(\"nb_explanations Stop iterations...\")\n",
    "                explanation_candidates = []  # stop algorithm\n",
    "                ## Found all the candidates\n",
    "                print(\n",
    "                    \"scores_candidates_to_expand_updated  \",\n",
    "                    scores_candidates_to_expand_updated,\n",
    "                )\n",
    "                # print(\"candidates_to_expand_updated   \", candidates_to_expand_updated)\n",
    "\n",
    "            elif len(candidates_to_expand_updated) != 0:\n",
    "                ## If there are possible candidates\n",
    "\n",
    "                explanation_candidates = []\n",
    "                it = 0  # Iteration of the while loop\n",
    "                indices = []\n",
    "\n",
    "                scores_candidates_to_expand2 = []\n",
    "                for score in scores_candidates_to_expand_updated:\n",
    "                    if score[0] < self.threshold_classifier:\n",
    "                        scores_candidates_to_expand2.append(2 * score_predicted)\n",
    "                    else:\n",
    "                        scores_candidates_to_expand2.append(score)\n",
    "                # update candidate scores if they have score less than threshold -> To expand them further\n",
    "                # shap_candidates_to_expand2 = []\n",
    "                # for candidate in candidates_to_expand_updated:\n",
    "                #     shapValues = 0\n",
    "                #     for word in candidate:\n",
    "                #         # find word in feature column in sorted_data\n",
    "                #         for ind in range(len(sorted_data_in)):\n",
    "                #             if sorted_data_in[ind][\"feature\"] == word:\n",
    "                #                 shapValues += sorted_data_in[ind][\"shapValue\"]\n",
    "                #                 break\n",
    "                #     shap_candidates_to_expand2.append(shapValues)\n",
    "\n",
    "                print(\n",
    "                    \"\\n scores_candidates_to_expand2 before loop\",\n",
    "                    scores_candidates_to_expand2,\n",
    "                )\n",
    "                print(\n",
    "                    len(explanation_candidates),\n",
    "                    it,\n",
    "                    \"<\",\n",
    "                    len(scores_candidates_to_expand2),\n",
    "                )\n",
    "\n",
    "                # *** WHILE LOOP ***\n",
    "                while (\n",
    "                    (len(explanation_candidates) == 0)\n",
    "                    and (it < len(scores_candidates_to_expand2))\n",
    "                    and ((time.time() - tic) < self.time_maximum)\n",
    "                ):\n",
    "                    # Stop if candidates are found or looped through more than there are candidates or maximum time reached\n",
    "\n",
    "                    print(\"While loop iteration %d\" % it)\n",
    "\n",
    "                    if it != 0:  # Because indices are not there in the first iteration\n",
    "                        for index in indices:\n",
    "                            scores_candidates_to_expand2[index] = 2 * score_predicted\n",
    "\n",
    "                    # print(\n",
    "                    #     \"\\n scores_candidates_to_expand2 after loop\",\n",
    "                    #     scores_candidates_to_expand2,\n",
    "                    # )\n",
    "                    # print(\"\\n indices\", indices)\n",
    "\n",
    "                    # do elementwise subtraction between score_predicted and scores_candidates_to_expand2\n",
    "                    subtractionList = []\n",
    "                    for item in scores_candidates_to_expand2:\n",
    "                        subtractionList.append(item - score_predicted)\n",
    "                    # for x, y in zip(score_predicted, scores_candidates_to_expand2):\n",
    "                    #     subtractionList.append(x - y)\n",
    "                    # print(\"subtractionList\", subtractionList)\n",
    "\n",
    "                    # Do element wise subtraction between the prediction score of the x_ref and every element of the scores_candidates_to_expand2\n",
    "                    index_combi_max = np.argmax(subtractionList)\n",
    "                    # index_shap_max = np.argmax(shap_candidates_to_expand2)\n",
    "                    # index_shap_min = np.argmin(shap_candidates_to_expand2)\n",
    "                    if self.revert == 0:\n",
    "                        index_combi_max = np.argmax(subtractionList)\n",
    "                    else:\n",
    "                        index_combi_max = np.argmin(subtractionList)\n",
    "                    # if self.revert == 0:\n",
    "                    #     index_combi_max = index_shap_max\n",
    "                    # else:\n",
    "                    #     index_combi_max = index_shap_min\n",
    "                    # print(\n",
    "                    #     \"subtrac max \",\n",
    "                    #     index_combi_max,\n",
    "                    #     \" index_shap_max \",\n",
    "                    #     index_shap_max,\n",
    "                    # )\n",
    "                    # Get the index of the maximum value -> Expand it\n",
    "                    print(\n",
    "                        \"\\n index_combi_max\",\n",
    "                        candidates_to_expand_updated[np.argmax(subtractionList)],\n",
    "                    )\n",
    "                    indices.append(index_combi_max)\n",
    "                    expanded_combis.append(\n",
    "                        candidates_to_expand_updated[index_combi_max]\n",
    "                    )\n",
    "                    # Add this combination to already expanded combinations as it will be expanded next by expand and prune function\n",
    "\n",
    "                    comb_to_expand = candidates_to_expand_updated[index_combi_max]\n",
    "                    replacement_comb_to_expand = (\n",
    "                        candidates_to_expand_updated_replacements[index_combi_max]\n",
    "                    )\n",
    "                    words_comb_selected = []\n",
    "                    for item in candidates_to_expand_updated[index_combi_max]:\n",
    "                        words_comb_selected.append(feature_names[item])\n",
    "                    print(\"The chosen combination is \", words_comb_selected)\n",
    "                    print_instance(\n",
    "                        conditional_replace_fn(\n",
    "                            candidates_to_expand_updated[index_combi_max],\n",
    "                            candidates_to_expand_updated_replacements[index_combi_max],\n",
    "                            instance.copy(),\n",
    "                        ),\n",
    "                        instance,\n",
    "                        self.feature_names,\n",
    "                    )\n",
    "                    print(\n",
    "                        \"It has a score of \",\n",
    "                        scores_candidates_to_expand_updated[index_combi_max],\n",
    "                    )\n",
    "                    # Expand the found combination with highest difference\n",
    "                    print(\"comb_to_expand\", comb_to_expand)\n",
    "                    print(\"replacement_comb_to_expand\", replacement_comb_to_expand)\n",
    "                    func = expand_and_prune(\n",
    "                        comb_to_expand,\n",
    "                        replacement_comb_to_expand,\n",
    "                        expanded_combis,\n",
    "                        feature_set,\n",
    "                        candidates_to_expand_updated,\n",
    "                        candidates_to_expand_updated_replacements,\n",
    "                        explanations_sets,\n",
    "                        explanation_replacement_sets,\n",
    "                        scores_candidates_to_expand_updated,\n",
    "                        instance,\n",
    "                        self.classifier_fn,\n",
    "                        self.revert,\n",
    "                        replacements,\n",
    "                    )\n",
    "                    \"\"\"Returns:\n",
    "                        - explanation_candidates: combinations of features that are explanation\n",
    "                        candidates to be checked in the next iteration\n",
    "                        - candidates_to_expand: combinations of features that are candidates to\n",
    "                        expanded in next iterations or candidates for \"best-first\"\n",
    "                        - expanded_combis: [list] list of combinations of features that are already\n",
    "                        expanded as \"best-first\"\n",
    "                        - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "                        combinations of features to be expanded\n",
    "                        - scores_explanation_candidates: scores after perturbation of explanation candidates\"\"\"\n",
    "                    explanation_candidates = func[0]\n",
    "                    explanation_candidates_replacements = func[1]\n",
    "                    candidates_to_expand = func[2]\n",
    "                    candidates_to_expand_replacements = func[3]\n",
    "                    expanded_combis = func[4]\n",
    "                    scores_candidates_to_expand = func[5]\n",
    "                    scores_explanation_candidates = func[6]\n",
    "\n",
    "                    it += 1\n",
    "\n",
    "                print(\n",
    "                    \"\\n\\n\\niteration - \", iteration, \" self.max_iter - \", self.max_iter\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(candidates_to_expand) - \",\n",
    "                    len(candidates_to_expand),\n",
    "                    \" != 0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(explanation_candidates) - \",\n",
    "                    len(explanation_candidates),\n",
    "                    \" !=0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\n(time.time() - tic) - \",\n",
    "                    (time.time() - tic),\n",
    "                    \" self.time_maximum - \",\n",
    "                    self.time_maximum,\n",
    "                )\n",
    "            print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** FINAL PART OF ALGORITHM ***\n",
    "        print(\"Iterations are done.\")\n",
    "\n",
    "        explanation_set = []\n",
    "        explanation_feature_names = []\n",
    "        index_of_min_length_explanation = -1\n",
    "        for i in range(len(explanations)):\n",
    "            explanation_feature_names = []\n",
    "            for features in explanations[i]:\n",
    "                explanation_feature_names.append(self.feature_names[features])\n",
    "            explanation_set.append(explanation_feature_names)\n",
    "\n",
    "        if len(explanations) != 0:\n",
    "            lengths_explanation = []\n",
    "            for explanation in explanations:\n",
    "                l = len(explanation)\n",
    "                lengths_explanation.append(l)\n",
    "            minimum_size_explanation = np.min(lengths_explanation)\n",
    "            index_of_min_length_explanation = np.argmin(lengths_explanation)\n",
    "        try:\n",
    "            print(\"argmin\", explanations[index_of_min_length_explanation])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(\"Final sentence\")\n",
    "        final_sentence = conditional_replace_fn(\n",
    "            explanations[index_of_min_length_explanation],\n",
    "            explanations_replacements[index_of_min_length_explanation],\n",
    "            instance.copy(),\n",
    "        )\n",
    "        final_sentence = print_instance(final_sentence, instance, self.feature_names)\n",
    "        new_instance = instance.copy()\n",
    "        new_replacements = []\n",
    "        replacement_features = []\n",
    "        for feature in explanations[index_of_min_length_explanation]:\n",
    "            feature_replacement = get_antonyms(\n",
    "                feature_names[feature], loaded_plain_model_rf\n",
    "            )\n",
    "            print(\"feature_replacement\", feature_replacement)\n",
    "            new_replacements.append(feature_replacement)\n",
    "        print(\"new_replacements\", new_replacements)\n",
    "        print(\"replacementfeature\", feature_names[new_replacements[0]])\n",
    "\n",
    "        output_removed_words = []\n",
    "        for item in explanations[index_of_min_length_explanation]:\n",
    "            output_removed_words.append(feature_names[item])\n",
    "        try:\n",
    "            replacementWords = []\n",
    "            for item_ind in range(len(new_replacements)):\n",
    "                replacementWords.append(\n",
    "                    {\n",
    "                        \"feature\": feature_names[\n",
    "                            explanations[index_of_min_length_explanation][item_ind]\n",
    "                        ],\n",
    "                        \"replacement\": feature_names[new_replacements[item_ind]][0],\n",
    "                    }\n",
    "                )\n",
    "            print(\"replacementWords\", replacementWords)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        new_insatnce = instance.copy()\n",
    "        index_of_min_length_explanation = -1\n",
    "        for relpacement_feature_index in range(len(new_replacements)):\n",
    "            if new_replacements[relpacement_feature_index] != []:\n",
    "                new_insatnce = replace_fn(\n",
    "                    x=explanations[index_of_min_length_explanation][\n",
    "                        relpacement_feature_index\n",
    "                    ],\n",
    "                    y=new_replacements[relpacement_feature_index],\n",
    "                    inst=new_insatnce,\n",
    "                )\n",
    "                replacement_features.append(\n",
    "                    feature_names[new_replacements[relpacement_feature_index]]\n",
    "                )\n",
    "            else:\n",
    "                new_insatnce = perturb_fn(\n",
    "                    explanations[index_of_min_length_explanation][\n",
    "                        relpacement_feature_index\n",
    "                    ],\n",
    "                    new_insatnce,\n",
    "                )\n",
    "                replacement_features.append(feature_names[relpacement_feature_index])\n",
    "        final_prob = self.classifier_fn(new_insatnce)\n",
    "        print(\"final_prob\", final_prob)\n",
    "\n",
    "        final_exp = []\n",
    "        for i in range(len(explanations[index_of_min_length_explanation])):\n",
    "            if new_replacements[i] != []:\n",
    "                final_exp.append(\n",
    "                    [output_removed_words[i], feature_names[new_replacements[i]][0]]\n",
    "                )\n",
    "            else:\n",
    "                final_exp.append([output_removed_words[i], \"---\"])\n",
    "\n",
    "        number_explanations = len(explanations)\n",
    "        if np.size(explanations_score_change) > 1:\n",
    "            inds = np.argsort(explanations_score_change, axis=0)\n",
    "            inds = np.fliplr([inds])[0]\n",
    "            inds_2 = []\n",
    "            for i in range(np.size(inds)):\n",
    "                inds_2.append(inds[i][0])\n",
    "            explanation_set_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanation_set_adjusted.append(explanation_set[j])\n",
    "            explanations_score_change_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanations_score_change_adjusted.append(explanations_score_change[j])\n",
    "            explanation_set = explanation_set_adjusted\n",
    "            explanations_score_change = explanations_score_change_adjusted\n",
    "\n",
    "        time_elapsed = time.time() - tic\n",
    "        print(\"\\n Total elapsed time %d \\n\" % time_elapsed)\n",
    "\n",
    "        indices_active_elements = np.nonzero(instance)[1]\n",
    "        # Find the elements in indices_active_elements_explain that are not in indices_active_elements\n",
    "        print(\"indices_active_elements\", indices_active_elements)\n",
    "\n",
    "        return {\n",
    "            \"final_exp\": final_exp,\n",
    "            \"number active elements\": number_active_elements,\n",
    "            \"number explanations found\": number_explanations,\n",
    "            \"size smallest explanation\": minimum_size_explanation,\n",
    "            \"time elapsed\": time_elapsed,\n",
    "            \"differences score\": explanations_score_change[0 : self.max_explained],\n",
    "            \"iterations\": iteration,\n",
    "            \"final_sentence\": final_sentence,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do no need\n",
    "class SEDC_Explainer_no(object):\n",
    "    \"\"\"Class for generating evidence counterfactuals for classifiers on behavioral/text data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_names,\n",
    "        classifier_fn,\n",
    "        threshold_classifier,\n",
    "        max_iter=100,\n",
    "        max_explained=1,\n",
    "        BB=True,\n",
    "        max_features=30,\n",
    "        time_maximum=120,\n",
    "        revert=0,\n",
    "    ):\n",
    "        \"\"\"Init function\n",
    "\n",
    "        Args:\n",
    "            classifier_fn: [function] classifier prediction probability function\n",
    "            or decision function. For ScikitClassifiers, this is classifier.predict_proba\n",
    "            or classifier.decision_function or classifier.predict_log_proba.\n",
    "            Make sure the function only returns one (float) value. For instance, if you\n",
    "            use a ScikitClassifier, transform the classifier.predict_proba as follows:\n",
    "\n",
    "                def classifier_fn(X):\n",
    "                    c=classification_model.predict_proba(X)\n",
    "                    y_predicted_proba=c[:,1]\n",
    "                    return y_predicted_proba\n",
    "\n",
    "            threshold_classifier: [float] the threshold that is used for classifying\n",
    "            instances as positive or not. When score or probability exceeds the\n",
    "            threshold value, then the instance is predicted as positive.\n",
    "            We have no default value, because it is important the user decides\n",
    "            a good value for the threshold.\n",
    "\n",
    "            feature_names: [numpy.array] contains the interpretable feature names,\n",
    "            such as the words themselves in case of document classification or the names\n",
    "            of visited URLs.\n",
    "\n",
    "            max_iter: [int] maximum number of iterations in the search procedure.\n",
    "            Default is set to 50.\n",
    "\n",
    "            max_explained: [int] maximum number of EDC explanations generated.\n",
    "            Default is set to 1.\n",
    "\n",
    "            BB: [“True” or “False”]  when the algorithm is augmented with\n",
    "            branch-and-bound (BB=True), one is only interested in the (set of)\n",
    "            shortest explanation(s). Default is \"True\".\n",
    "\n",
    "            max_features: [int] maximum number of features allowed in the explanation(s).\n",
    "            Default is set to 30.\n",
    "\n",
    "            time_maximum: [int] maximum time allowed to generate explanations,\n",
    "            expressed in minutes. Default is set to 2 minutes (120 seconds).\n",
    "        \"\"\"\n",
    "\n",
    "        self.feature_names = feature_names\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.threshold_classifier = threshold_classifier\n",
    "        self.max_iter = max_iter\n",
    "        self.max_explained = max_explained\n",
    "        self.BB = BB\n",
    "        self.max_features = max_features\n",
    "        self.time_maximum = time_maximum\n",
    "        self.revert = None\n",
    "        self.initial_class = None\n",
    "\n",
    "    def explanation(self, instance):\n",
    "        \"\"\"Generates evidence counterfactual explanation for the instance.\n",
    "        ONLY IF THE CURRENT INSTANCE IS POSITIVE -> Limitation\n",
    "\n",
    "        Args:\n",
    "            instance: [numpy.array or sparse matrix] instance to explain\n",
    "\n",
    "        Returns:\n",
    "            A dictionary where:\n",
    "\n",
    "                explanation_set: explanation(s) ranked from high to low change\n",
    "                in predicted score or probability.\n",
    "                The number of explanations shown depends on the argument max_explained.\n",
    "\n",
    "                number_active_elements: number of active elements of\n",
    "                the instance of interest.\n",
    "\n",
    "                number_explanations: number of explanations found by algorithm.\n",
    "\n",
    "                minimum_size_explanation: number of features in the smallest explanation.\n",
    "\n",
    "                time_elapsed: number of seconds passed to generate explanation(s).\n",
    "\n",
    "                explanations_score_change: change in predicted score/probability\n",
    "                when removing the features in the explanation, ranked from\n",
    "                high to low change.\n",
    "        \"\"\"\n",
    "\n",
    "        # *** INITIALIZATION ***\n",
    "        print(\"Start initialization...\")\n",
    "        tic = time.time()\n",
    "        instance = lil_matrix(instance)\n",
    "        iteration = 0\n",
    "        nb_explanations = 0\n",
    "        minimum_size_explanation = np.nan\n",
    "        explanations = []\n",
    "        explanations_sets = []\n",
    "        explanations_score_change = []\n",
    "        expanded_combis = []\n",
    "        score_predicted = self.classifier_fn(instance)  ## Returns Prediction Prob\n",
    "        # Intial class is 1 is score is greater than threshold\n",
    "        if score_predicted > self.threshold_classifier:\n",
    "            self.initial_class = [1]\n",
    "        else:\n",
    "            self.initial_class = [0]\n",
    "            self.revert = 1\n",
    "        print(\n",
    "            \"score_predicted  \",\n",
    "            score_predicted,\n",
    "            \"  initial_class  \",\n",
    "            self.initial_class,\n",
    "        )\n",
    "\n",
    "        reference = np.reshape(\n",
    "            np.zeros(np.shape(instance)[1]), (1, len(np.zeros(np.shape(instance)[1])))\n",
    "        )\n",
    "        reference = sparse.csr_matrix(reference)\n",
    "\n",
    "        explainer = shap.KernelExplainer(self.classifier_fn, reference, link=\"identity\")\n",
    "        shapVals = explainer.shap_values(instance, nsamples=5000, l1_reg=\"aic\")\n",
    "\n",
    "        features = []\n",
    "        for ind in range(len(shapVals[0])):\n",
    "            if shapVals[0, ind] != 0:\n",
    "                features.append({\"feature\": ind, \"shapValue\": shapVals[0, ind]})\n",
    "        sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"], reverse=True)\n",
    "        inverse_sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"])\n",
    "\n",
    "        if self.revert == 1:\n",
    "            sorted_data_in = inverse_sorted_data_in\n",
    "\n",
    "        indices_active_elements = np.nonzero(instance)[\n",
    "            1\n",
    "        ]  ## -> Gets non zero elements in the instance as an array [x, y, z]\n",
    "        sorted_indices = sorted(\n",
    "            indices_active_elements, key=lambda x: shapVals[0, x], reverse=True\n",
    "        )\n",
    "        indices_active_elements = np.array(sorted_indices)\n",
    "        number_active_elements = len(indices_active_elements)\n",
    "        indices_active_elements = indices_active_elements.reshape(\n",
    "            (number_active_elements, 1)\n",
    "        )  ## -> Reshape to get a predictable\n",
    "\n",
    "        candidates_to_expand = (\n",
    "            []\n",
    "        )  # -> These combinations are further expanded -> These are the elements to be removed from the sentence\n",
    "        for features in indices_active_elements:\n",
    "            candidates_to_expand.append(OrderedSet(features))\n",
    "        print(\"candidates_to_expand \", candidates_to_expand)\n",
    "        ## > Gets an array with each element in reshaped incides as an ordered set -> [OrderedSet([430]), OrderedSet([588]), OrderedSet([595])]\n",
    "\n",
    "        explanation_candidates = candidates_to_expand.copy()\n",
    "        print(\"explanation_candidates \", explanation_candidates)\n",
    "        ## Gets a copy of the above array -> Initially\n",
    "\n",
    "        feature_set = [\n",
    "            frozenset(x) for x in indices_active_elements\n",
    "        ]  ## Immutable -> can be used as keys in dictionary\n",
    "        ## Used features in the current x-reference -> incides of the words in the review.\n",
    "\n",
    "        print(\"Initialization is complete.\")\n",
    "        print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** WHILE LOOP ***\n",
    "        while (\n",
    "            (iteration < self.max_iter)\n",
    "            and (nb_explanations < self.max_explained)\n",
    "            and (len(candidates_to_expand) != 0)\n",
    "            and (len(explanation_candidates) != 0)\n",
    "            and ((time.time() - tic) < self.time_maximum)\n",
    "        ):\n",
    "            ## Stop if maximum iterations exceeded\n",
    "            #  number of explanations generated is greater than the maximum explanations\n",
    "            #  There are no candidates to expand\n",
    "            #  There are no explanation candidates -> Used to force stop while loop below\n",
    "            #  Or maximum allowed time exceeded\n",
    "            iteration += 1\n",
    "            print(\"\\n Iteration %d \\n\" % iteration)\n",
    "\n",
    "            if iteration == 1:\n",
    "                print(\"Run in first iteration -> perturbation done \\n\")\n",
    "                # Print the word in each index in the explanation candidates\n",
    "                # for item in explanation_candidates:\n",
    "                #     print([self.feature_names[x] for x in item])\n",
    "                replacements = [\n",
    "                    get_antonyms(self.feature_names[x[0]], loaded_plain_model_rf)\n",
    "                    for x in explanation_candidates\n",
    "                ]\n",
    "                # convert each element in replacement to a OrderedSet\n",
    "                replacements = [OrderedSet(x) for x in replacements]\n",
    "                print(\"replacements \\n\", replacements, \"\\n\")\n",
    "                print(\"explanation_candidates \\n\", explanation_candidates, \"\\n\")\n",
    "                perturbed_instances = [\n",
    "                    perturb_fn(x, inst=instance.copy()) for x in explanation_candidates\n",
    "                ]\n",
    "                replaced_instances = []\n",
    "                for i in range(len(explanation_candidates)):\n",
    "                    if replacements[i] == OrderedSet():\n",
    "                        replaced_instances.append(\n",
    "                            perturb_fn(\n",
    "                                x=explanation_candidates[i], inst=instance.copy()\n",
    "                            )\n",
    "                        )\n",
    "                    else:\n",
    "                        replaced_instances.append(\n",
    "                            replace_fn(\n",
    "                                x=explanation_candidates[i],\n",
    "                                y=replacements[i],\n",
    "                                inst=instance.copy(),\n",
    "                            )\n",
    "                        )\n",
    "                print(\"replaced_instances \\n\", replaced_instances, \"\\n\")\n",
    "                # Remove the elements in the indices given by the ordered set x and return an array fo such elements\n",
    "                # Removes only one element in the first run -> Contains sentences with one word removed\n",
    "                perturbed_instances = replaced_instances\n",
    "                scores_explanation_candidates = [\n",
    "                    self.classifier_fn(x, self.revert) for x in perturbed_instances\n",
    "                ]\n",
    "                # Get predictions for each perturbed instance where one or more elements are removed from the initial instance\n",
    "                # It is in form of [[x], [y], [z]]\n",
    "                print(\n",
    "                    \"scores_explanation_candidates \\n\",\n",
    "                    scores_explanation_candidates,\n",
    "                    \"\\n\",\n",
    "                )\n",
    "                scores_candidates_to_expand = scores_explanation_candidates.copy()\n",
    "\n",
    "            scores_perturbed_new_combinations = [\n",
    "                x[0] for x in scores_explanation_candidates\n",
    "            ]\n",
    "            # Therefore get it to the shape [x, y, z] by getting the [0] th element of each element array\n",
    "            # print(\n",
    "            #     \"scores_perturbed_new_combinations \", scores_perturbed_new_combinations\n",
    "            # )\n",
    "\n",
    "            # ***CHECK IF THERE ARE EXPLANATIONS***\n",
    "            new_explanations = list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # Get explanation candidates where their probability is less than the threshold classifier -> Positive becomes negative\n",
    "            # print(\"New Explanations \\n\", new_explanations)\n",
    "            explanations += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # print(\"\\n explanations, explanations_score_change\", explanations)\n",
    "            nb_explanations += len(\n",
    "                list(\n",
    "                    compress(\n",
    "                        explanation_candidates,\n",
    "                        scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                    )\n",
    "                )\n",
    "            )  # Update number of explanations which pass the required threshold\n",
    "            explanations_sets += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            explanations_sets = [\n",
    "                set(x) for x in explanations_sets\n",
    "            ]  # Convert each array to a set -> to get the words\n",
    "            explanations_score_change += list(\n",
    "                compress(\n",
    "                    scores_explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # print('explanations_score_change', explanations_score_change)\n",
    "\n",
    "            # Adjust max_length\n",
    "            if self.BB == True:\n",
    "                if len(explanations) != 0:\n",
    "                    lengths = []  # Record length of each explanation found\n",
    "                    for explanation in explanations:\n",
    "                        lengths.append(len(explanation))\n",
    "                    lengths = np.array(lengths)\n",
    "                    max_length = lengths.min()\n",
    "                    # Get minimum length of the found explanations as max length -> Do not search for explanations with longer length\n",
    "                else:\n",
    "                    max_length = number_active_elements  # Else can find maximum length equal to number of words in instance\n",
    "            else:\n",
    "                max_length = number_active_elements\n",
    "            print(\"\\n-------------Max length updated to - \", max_length)\n",
    "\n",
    "            # Eliminate combinations from candidates_to_expand (\"best-first\" candidates) that can not be expanded\n",
    "            # Pruning based on Branch & Bound=True, max. features allowed and number of active features\n",
    "            candidates_to_expand_updated = []\n",
    "            scores_candidates_to_expand_updated = (\n",
    "                []\n",
    "            )  # enumerate -> Find count of || to list one after another\n",
    "            for j, combination in enumerate(candidates_to_expand):\n",
    "                if (\n",
    "                    (len(combination) < number_active_elements)\n",
    "                    and (len(combination) < max_length)\n",
    "                    and (len(combination) < self.max_features)\n",
    "                ):\n",
    "                    # Combination length should be less than the words in the input and max length of the required explanation and required maximum features\n",
    "                    candidates_to_expand_updated.append(\n",
    "                        combination\n",
    "                    )  # If the combination matches, it is further expanded\n",
    "                    scores_candidates_to_expand_updated.append(\n",
    "                        scores_candidates_to_expand[j]\n",
    "                    )\n",
    "                    # Add the prediction score to the new array\n",
    "                    # get the score from the scores_candidates_to_expand using the current index\n",
    "\n",
    "            print(\n",
    "                \"\\nlen(candidates_to_expand_updated)\",\n",
    "                len(candidates_to_expand_updated),\n",
    "                \" 0 \",\n",
    "            )\n",
    "            print(\n",
    "                \"\\nnb_explanations\",\n",
    "                nb_explanations,\n",
    "                \" >= self.max_explained \",\n",
    "                self.max_explained,\n",
    "            )\n",
    "\n",
    "            # *** IF LOOP ***\n",
    "            # expanding the candidates to update will exceed the max length set in the earlier loop\n",
    "            if (len(candidates_to_expand_updated) == 0) or (\n",
    "                nb_explanations >= self.max_explained\n",
    "            ):\n",
    "                ## If the number of explanations exceeded the required number\n",
    "                ## or no candidates\n",
    "                ## no explanations present\n",
    "\n",
    "                print(\"nb_explanations Stop iterations...\")\n",
    "                explanation_candidates = []  # stop algorithm\n",
    "                ## Found all the candidates\n",
    "                print(\n",
    "                    \"scores_candidates_to_expand_updated  \",\n",
    "                    scores_candidates_to_expand_updated,\n",
    "                )\n",
    "                # print(\"candidates_to_expand_updated   \", candidates_to_expand_updated)\n",
    "\n",
    "            elif len(candidates_to_expand_updated) != 0:\n",
    "                ## If there are possible candidates\n",
    "                print(\"elif\", len(candidates_to_expand_updated), \" != 0\")\n",
    "\n",
    "                explanation_candidates = []\n",
    "                it = 0  # Iteration of the while loop\n",
    "                indices = []\n",
    "\n",
    "                scores_candidates_to_expand2 = []\n",
    "                for score in scores_candidates_to_expand_updated:\n",
    "                    if score[0] < self.threshold_classifier:\n",
    "                        scores_candidates_to_expand2.append(2 * score_predicted)\n",
    "                    else:\n",
    "                        scores_candidates_to_expand2.append(score)\n",
    "                # update candidate scores if they have score less than threshold -> To expand them further\n",
    "                shap_candidates_to_expand2 = []\n",
    "                for candidate in candidates_to_expand_updated:\n",
    "                    shapValues = 0\n",
    "                    for word in candidate:\n",
    "                        # find word in feature column in sorted_data\n",
    "                        for ind in range(len(sorted_data_in)):\n",
    "                            if sorted_data_in[ind][\"feature\"] == word:\n",
    "                                shapValues += sorted_data_in[ind][\"shapValue\"]\n",
    "                                break\n",
    "                    shap_candidates_to_expand2.append(shapValues)\n",
    "\n",
    "                # print(\n",
    "                #     \"\\n scores_candidates_to_expand2 before loop\",\n",
    "                #     scores_candidates_to_expand2,\n",
    "                # )\n",
    "\n",
    "                # *** WHILE LOOP ***\n",
    "                while (\n",
    "                    (len(explanation_candidates) == 0)\n",
    "                    and (it < len(scores_candidates_to_expand2))\n",
    "                    and ((time.time() - tic) < self.time_maximum)\n",
    "                ):\n",
    "                    # Stop if candidates are found or looped through more than there are candidates or maximum time reached\n",
    "\n",
    "                    print(\"While loop iteration %d\" % it)\n",
    "\n",
    "                    if it != 0:  # Because indices are not there in the first iteration\n",
    "                        for index in indices:\n",
    "                            scores_candidates_to_expand2[index] = 2 * score_predicted\n",
    "\n",
    "                    # print(\n",
    "                    #     \"\\n scores_candidates_to_expand2 after loop\",\n",
    "                    #     scores_candidates_to_expand2,\n",
    "                    # )\n",
    "                    # print(\"\\n indices\", indices)\n",
    "\n",
    "                    # do elementwise subtraction between score_predicted and scores_candidates_to_expand2\n",
    "                    subtractionList = []\n",
    "                    for x, y in zip(score_predicted, scores_candidates_to_expand2):\n",
    "                        print(\"\\n x, y\", x - y)\n",
    "                        subtractionList.append(x - y)\n",
    "\n",
    "                    # Do element wise subtraction between the prediction score of the x_ref and every element of the scores_candidates_to_expand2\n",
    "                    index_combi_max = np.argmax(subtractionList)\n",
    "                    if self.revert == 0:\n",
    "                        index_combi_max = np.argmax(subtractionList)\n",
    "                    else:\n",
    "                        index_combi_max = np.argmin(subtractionList)\n",
    "                    # index_shap_max = np.argmax(shap_candidates_to_expand2)\n",
    "                    # index_shap_min = np.argmin(shap_candidates_to_expand2)\n",
    "                    # if self.revert == 0:\n",
    "                    #     index_combi_max = index_shap_max\n",
    "                    # else:\n",
    "                    #     index_combi_max = index_shap_min\n",
    "                    # print(\n",
    "                    #     \"subtrac max \",\n",
    "                    #     index_combi_max,\n",
    "                    #     \" index_shap_max \",\n",
    "                    #     index_shap_max,\n",
    "                    # )\n",
    "                    # # Get the index of the maximum value -> Expand it\n",
    "                    # print(\n",
    "                    #     \"\\n index_combi_max\",\n",
    "                    #     candidates_to_expand_updated[np.argmax(subtractionList)],\n",
    "                    #     \"\\n index_shap_max\",\n",
    "                    #     candidates_to_expand_updated[index_combi_max],\n",
    "                    # )\n",
    "                    indices.append(index_combi_max)\n",
    "                    expanded_combis.append(\n",
    "                        candidates_to_expand_updated[index_combi_max]\n",
    "                    )\n",
    "                    # Add this combination to already expanded combinations as it will be expanded next by expand and prune function\n",
    "\n",
    "                    comb_to_expand = candidates_to_expand_updated[index_combi_max]\n",
    "                    # Expand the found combination with highest difference\n",
    "                    func = expand_and_prune(\n",
    "                        comb_to_expand,\n",
    "                        expanded_combis,\n",
    "                        feature_set,\n",
    "                        candidates_to_expand_updated,\n",
    "                        explanations_sets,\n",
    "                        scores_candidates_to_expand_updated,\n",
    "                        instance,\n",
    "                        self.classifier_fn,\n",
    "                        self.revert,\n",
    "                    )\n",
    "                    \"\"\"Returns:\n",
    "                        - explanation_candidates: combinations of features that are explanation\n",
    "                        candidates to be checked in the next iteration\n",
    "                        - candidates_to_expand: combinations of features that are candidates to\n",
    "                        expanded in next iterations or candidates for \"best-first\"\n",
    "                        - expanded_combis: [list] list of combinations of features that are already\n",
    "                        expanded as \"best-first\"\n",
    "                        - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "                        combinations of features to be expanded\n",
    "                        - scores_explanation_candidates: scores after perturbation of explanation candidates\"\"\"\n",
    "                    explanation_candidates = func[0]\n",
    "                    candidates_to_expand = func[1]\n",
    "                    expanded_combis = func[2]\n",
    "                    scores_candidates_to_expand = func[3]\n",
    "                    scores_explanation_candidates = func[4]\n",
    "\n",
    "                    it += 1\n",
    "\n",
    "                print(\n",
    "                    \"\\n\\n\\niteration - \", iteration, \" self.max_iter - \", self.max_iter\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(candidates_to_expand) - \",\n",
    "                    len(candidates_to_expand),\n",
    "                    \" != 0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(explanation_candidates) - \",\n",
    "                    len(explanation_candidates),\n",
    "                    \" !=0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\n(time.time() - tic) - \",\n",
    "                    (time.time() - tic),\n",
    "                    \" self.time_maximum - \",\n",
    "                    self.time_maximum,\n",
    "                )\n",
    "            print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** FINAL PART OF ALGORITHM ***\n",
    "        print(\"Iterations are done.\")\n",
    "\n",
    "        explanation_set = []\n",
    "        explanation_feature_names = []\n",
    "        for i in range(len(explanations)):\n",
    "            explanation_feature_names = []\n",
    "            for features in explanations[i]:\n",
    "                explanation_feature_names.append(self.feature_names[features])\n",
    "            explanation_set.append(explanation_feature_names)\n",
    "\n",
    "        if len(explanations) != 0:\n",
    "            lengths_explanation = []\n",
    "            for explanation in explanations:\n",
    "                l = len(explanation)\n",
    "                lengths_explanation.append(l)\n",
    "            minimum_size_explanation = np.min(lengths_explanation)\n",
    "\n",
    "        number_explanations = len(explanations)\n",
    "        if np.size(explanations_score_change) > 1:\n",
    "            inds = np.argsort(explanations_score_change, axis=0)\n",
    "            inds = np.fliplr([inds])[0]\n",
    "            inds_2 = []\n",
    "            for i in range(np.size(inds)):\n",
    "                inds_2.append(inds[i][0])\n",
    "            explanation_set_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanation_set_adjusted.append(explanation_set[j])\n",
    "            explanations_score_change_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanations_score_change_adjusted.append(explanations_score_change[j])\n",
    "            explanation_set = explanation_set_adjusted\n",
    "            explanations_score_change = explanations_score_change_adjusted\n",
    "\n",
    "        time_elapsed = time.time() - tic\n",
    "        print(\"\\n Total elapsed time %d \\n\" % time_elapsed)\n",
    "\n",
    "        print(\n",
    "            \"If we remove the words \",\n",
    "            explanation_set[0 : self.max_explained],\n",
    "            \"From the review, the prediction will be reversed\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"explanation set\": explanation_set[0 : self.max_explained],\n",
    "            \"number active elements\": number_active_elements,\n",
    "            \"number explanations found\": number_explanations,\n",
    "            \"size smallest explanation\": minimum_size_explanation,\n",
    "            \"time elapsed\": time_elapsed,\n",
    "            \"differences score\": explanations_score_change[0 : self.max_explained],\n",
    "            \"iterations\": iteration,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52259f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:14.775696Z",
     "iopub.status.busy": "2023-05-23T22:24:14.775084Z",
     "iopub.status.idle": "2023-05-23T22:24:22.763555Z",
     "shell.execute_reply": "2023-05-23T22:24:22.762523Z"
    },
    "papermill": {
     "duration": 8.009867,
     "end_time": "2023-05-23T22:24:22.767023",
     "exception": false,
     "start_time": "2023-05-23T22:24:14.757156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get threshold_classifier_probs\n",
    "p = np.sum(y_train_imdb) / np.size(y_train_imdb)\n",
    "\n",
    "probs = loaded_plain_model_lr.predict(x_test_imdb)\n",
    "threshold_classifier_probs = np.percentile(probs, (50.41))\n",
    "print(threshold_classifier_probs)\n",
    "predictions_probs = probs >= threshold_classifier_probs\n",
    "\n",
    "accuracy_test = accuracy_score(y_test_imdb, np.array(predictions_probs))\n",
    "print(\"The accuracy of the model on the test data is %f\" % accuracy_test)\n",
    "\n",
    "# indices_probs_pos = np.nonzero(predictions_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426353cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not need\n",
    "for i in range(x_test_imdb.shape[0]):\n",
    "    counter = 0\n",
    "    if round(classifier_fn_lr(x_test_imdb[i, :])[0], 1) == 0.6:\n",
    "        counter = counter + 1\n",
    "        print(i)\n",
    "    if counter > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf2da0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.8710484661364114\n",
      "18 0.8183767362908165\n",
      "34 0.8526536137479714\n",
      "53 0.8851770198684602\n",
      "60 0.8385767256610298\n",
      "63 0.8927611936112447\n",
      "64 0.8666939779029769\n",
      "70 0.8170098160732301\n"
     ]
    }
   ],
   "source": [
    "indices_arr = []\n",
    "for index in range(100):\n",
    "    score = classifier_fn_lr(x_test_imdb[index, :])[0]\n",
    "    if score < 0.9 and score > 0.8:\n",
    "        indices_arr.append(index)\n",
    "        print(index, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08552c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:22.905668Z",
     "iopub.status.busy": "2023-05-23T22:24:22.903844Z",
     "iopub.status.idle": "2023-05-23T22:24:22.911474Z",
     "shell.execute_reply": "2023-05-23T22:24:22.910280Z"
    },
    "papermill": {
     "duration": 0.028079,
     "end_time": "2023-05-23T22:24:22.914460",
     "exception": false,
     "start_time": "2023-05-23T22:24:22.886381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer_shap = SEDC_Explainer(\n",
    "    feature_names=feature_names,\n",
    "    threshold_classifier=threshold_classifier_probs,\n",
    "    classifier_fn=classifier_fn_lr,\n",
    "    max_iter=50,\n",
    "    time_maximum=120,\n",
    ")\n",
    "\n",
    "\n",
    "explanation_normal = explainer_shap.explanation(x_test_imdb[10, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.097894,
   "end_time": "2023-05-23T22:25:02.784322",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-23T22:22:55.686428",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
