{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fc1f8d7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-23T22:23:11.513543Z",
     "iopub.status.busy": "2023-05-23T22:23:11.512957Z",
     "iopub.status.idle": "2023-05-23T22:23:13.539048Z",
     "shell.execute_reply": "2023-05-23T22:23:13.537538Z"
    },
    "papermill": {
     "duration": 2.04584,
     "end_time": "2023-05-23T22:23:13.542144",
     "exception": false,
     "start_time": "2023-05-23T22:23:11.496304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n"
     ]
    }
   ],
   "source": [
    "## IMPORTS\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('omw-1.4')\n",
    "# ##!unzip C:/Users/TharushaLekamge/AppData/Roaming/nltk_data/corpora/wordnet.zip -d C:/Users/TharushaLekamge/AppData/Roaming/nltk_data/corpora/\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Importing Shap for shapley values\n",
    "import shap\n",
    "\n",
    "from ordered_set import OrderedSet\n",
    "from scipy.sparse import lil_matrix\n",
    "from itertools import compress\n",
    "\n",
    "loaded_vocab = pickle.load(open('../modelExports/vectorizer_imdb.pkl', 'rb'))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = nltk.tokenize.toktok.ToktokTokenizer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "loaded_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=2, vocabulary=loaded_vocab)\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "\n",
    "# Load Models -> Preprocess data\n",
    "\n",
    "loaded_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=2, vocabulary=loaded_vocab)\n",
    "loaded_plain_model_rf = pickle.load(open('..\\modelExports\\grid_imdb_rf.pickle', \"rb\"))\n",
    "loaded_plain_model_svc = pickle.load(open('..\\modelExports\\grid_imdb_svc.pickle', \"rb\"))\n",
    "loaded_plain_model_lr = pickle.load(open('..\\modelExports\\grid_imdb_lr.pickle', \"rb\"))\n",
    "loaded_plain_model_knn = pickle.load(open('..\\modelExports\\grid_imdb_knn.pickle', \"rb\"))\n",
    "\n",
    "## Support functions\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words=word_tokenize(text)\n",
    "    edited_text = ''\n",
    "    for word in words:\n",
    "        lemma_word=lemmatizer.lemmatize(word)\n",
    "        extra=\" \"+str(lemma_word)\n",
    "        edited_text+=extra\n",
    "    return edited_text\n",
    "\n",
    "def get_antonyms(word, model):\n",
    "    \"\"\"\" Get antonyms of a word and their indices in the feature vector\n",
    "    Args: \n",
    "        word: word to get antonyms for\n",
    "        model: trained model with feature_importances_\n",
    "\n",
    "    Returns:\n",
    "        tuple of antonyms and their indices in the feature vector\n",
    "    \"\"\"\n",
    "    antonyms = []\n",
    "    antonyms_indices = []\n",
    "    feature_importance = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "            if i.antonyms():\n",
    "                antonyms.append(i.antonyms()[0].name())\n",
    "    # Remove duplicates in antonyms\n",
    "    antonyms = list(set(antonyms))\n",
    "\n",
    "    for word in antonyms:\n",
    "        if word in loaded_vocab:\n",
    "            antonyms_indices.append(loaded_vocab[word])\n",
    "            feature_importance.append(\n",
    "                model.feature_importances_[loaded_vocab[word]])\n",
    "    # Sort the antonyms and their indices based on feature importance\n",
    "    antonyms_indices = [x for _, x in sorted(\n",
    "        zip(feature_importance, antonyms_indices), reverse=True)]\n",
    "    antonyms = [x for _, x in sorted(\n",
    "        zip(feature_importance, antonyms), reverse=True)]\n",
    "    if len(antonyms_indices) > 0:\n",
    "        return [antonyms_indices[0]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "## GET INPUT DATA\n",
    "data = pd.read_csv('../modelExports/IMDB-Dataset.csv')\n",
    "data = data.sample(10000)\n",
    "\n",
    "feature_names = loaded_vectorizer.get_feature_names_out()\n",
    "feature_importances = loaded_plain_model_rf.best_estimator_.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fe6fbb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:23:13.616410Z",
     "iopub.status.busy": "2023-05-23T22:23:13.615606Z",
     "iopub.status.idle": "2023-05-23T22:23:16.332440Z",
     "shell.execute_reply": "2023-05-23T22:23:16.330586Z"
    },
    "papermill": {
     "duration": 2.736958,
     "end_time": "2023-05-23T22:23:16.336104",
     "exception": false,
     "start_time": "2023-05-23T22:23:13.599146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n"
     ]
    }
   ],
   "source": [
    "## Preprocess text\n",
    "\n",
    "data.review = data.review.str.lower()\n",
    "data.review = data.review.apply(strip_html)\n",
    "data.review = data.review.apply(remove_special_characters)\n",
    "data.review = data.review.apply(remove_stopwords)\n",
    "data.review = data.review.apply(lemmatize_text)\n",
    "\n",
    "## Split Data\n",
    "x_imdb = data['review']\n",
    "y_imdb = data['sentiment']\n",
    "\n",
    "x_train_i, x_test_i, y_train_i, y_test_i = train_test_split(x_imdb,y_imdb,test_size=0.2)\n",
    "x_test, x_val, y_test_i, y_val_i = train_test_split(x_test_i,y_test_i,test_size=0.5)\n",
    "\n",
    "x_train_imdb = loaded_vectorizer.fit_transform(x_train_i)\n",
    "x_test_imdb = loaded_vectorizer.transform(x_test)\n",
    "x_val_imdb = loaded_vectorizer.transform(x_val)\n",
    "\n",
    "# Binarize y - Positive is 1\n",
    "y_train_imdb = label_binarizer.fit_transform(y_train_i)\n",
    "y_test_imdb = label_binarizer.fit_transform(y_test_i)\n",
    "y_val_imdb = label_binarizer.fit_transform(y_val_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580642f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Forest\n",
    "sample_id = 0\n",
    "positiveCount = 0\n",
    "negativeCount = 0\n",
    "X_train = x_train_imdb\n",
    "\n",
    "for j, tree in enumerate(loaded_plain_model_rf.best_estimator_.estimators_):\n",
    "    print('----------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left # Left child of node j -> access the left child by children_left[j]\n",
    "    children_right = tree.tree_.children_right  # Right child of node j -> access the right child by children_right[j]\n",
    "    feature = tree.tree_.feature # Stores features used in each node j -> access the feature by feature[j]\n",
    "    threshold = tree.tree_.threshold    # Stores the threshold value at node j -> access the threshold by threshold[j]\n",
    "\n",
    "    print(\"Decision path for DecisionTree {0}\".format(j))\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "\n",
    "    node_indicator = tree.decision_path(X_train[0])\n",
    "    leave_id = tree.apply(X_train)\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                        node_indicator.indptr[sample_id + 1]] # Indices of nodes visited by sample_id in the current tree\n",
    "\n",
    "\n",
    "    print('Leave id: ', leave_id[sample_id])\n",
    "    print('    Rules used to predict sample %s, node index : ' % (sample_id))\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split\n",
    "        # node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack`\n",
    "        # so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print(\n",
    "        \"The binary tree structure has {n} nodes and has \"\n",
    "        \"the following tree structure:\\n\".format(n=n_nodes)\n",
    "    )\n",
    "    for i in range(n_nodes):\n",
    "        # if i is in node_index, then this is a node we care about\n",
    "        if i in node_index:\n",
    "            if is_leaves[i]:\n",
    "                if tree.tree_.value[i][0][0] > tree.tree_.value[i][0][1]:\n",
    "                    positiveCount += 1\n",
    "                else:\n",
    "                    negativeCount += 1\n",
    "                print(\n",
    "                    \"{space}node={node} is a leaf node with value {value}.\".format(\n",
    "                        space=node_depth[i] * \"\\t\", node=i, value=tree.tree_.value[i]\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"{space}node={node} is a split node: \"\n",
    "                    \"go to node {left} if X[:, {feature} {name}] <= {threshold} \"\n",
    "                    \"else to node {right}.\".format(\n",
    "                        space=node_depth[i] * \"\\t\",\n",
    "                        node=i,\n",
    "                        left=children_left[i],\n",
    "                        feature=feature[i],\n",
    "                        name=feature_names[feature[i]],\n",
    "                        threshold=threshold[i],\n",
    "                        right=children_right[i],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "print('Positive Count: ', positiveCount), print('Negative Count: ', negativeCount)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5b5b511",
   "metadata": {
    "papermill": {
     "duration": 0.014519,
     "end_time": "2023-05-23T22:23:57.584948",
     "exception": false,
     "start_time": "2023-05-23T22:23:57.570429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SEDC Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f51db9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:14.202042Z",
     "iopub.status.busy": "2023-05-23T22:24:14.201564Z",
     "iopub.status.idle": "2023-05-23T22:24:14.231018Z",
     "shell.execute_reply": "2023-05-23T22:24:14.229850Z"
    },
    "papermill": {
     "duration": 0.050135,
     "end_time": "2023-05-23T22:24:14.234197",
     "exception": false,
     "start_time": "2023-05-23T22:24:14.184062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perturb_fn(x,inst):\n",
    "    \"\"\" Function to perturb instance x -> Deform the array -> assign 0 to the x-th column \"\"\"\n",
    "    \"\"\"\n",
    "    Returns perturbed instance inst\n",
    "    \"\"\"\n",
    "    inst[:,x]=0\n",
    "    return inst\n",
    "\n",
    "def replace_fn(x,y,inst):\n",
    "    \"\"\" Function to perturb instance x -> Deform the array -> assign 0 to the x-th column \"\"\"\n",
    "    \"\"\"\n",
    "    Returns perturbed instance inst\n",
    "    \"\"\"\n",
    "    temp_x = inst[:,x]\n",
    "    temp_y = inst[:,y]\n",
    "    inst[:,x] = temp_y\n",
    "    inst[:,y] = temp_x\n",
    "    return inst\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    - comb: \"best-first\" (combination of) feature(s) that is expanded\n",
    "    (e.g., comb_to_expand)\n",
    "    - expanded_combis: list of combinations of features that are already \n",
    "    expanded as \"best-first\"\n",
    "    - feature_set: indices of the active features of the instance \n",
    "    - candidates_to_expand: combinations of features that are candidates to be \n",
    "    expanded in next iterations or candidates for \"best-first\"\n",
    "    - explanations_sets: counterfactual explanations already found\n",
    "    - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "    combinations of features to be expanded\n",
    "    - instance: instance to be explained\n",
    "    - cf: classifier prediction probability function\n",
    "    or decision function. For ScikitClassifiers, this is classifier.predict_proba \n",
    "    or classifier.decision_function or classifier.predict_log_proba.\n",
    "    Make sure the function only returns one (float) value. For instance, if you\n",
    "    use a ScikitClassifier, transform the classifier.predict_proba as follows:\n",
    "            \n",
    "        def classifier_fn(X):\n",
    "            c=classification_model.predict_proba(X)\n",
    "            y_predicted_proba=c[:,1]\n",
    "            return y_predicted_proba\n",
    "    \n",
    "Returns:\n",
    "    - explanation_candidates: combinations of features that are explanation\n",
    "    candidates to be checked in the next iteration\n",
    "    - candidates_to_expand: combinations of features that are candidates to be \n",
    "    expanded in next iterations or candidates for \"best-first\"\n",
    "    - expanded_combis: [list] list of combinations of features that are already \n",
    "    expanded as \"best-first\"    \n",
    "    - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "    combinations of features to be expanded\n",
    "    - scores_explanation_candidates: scores after perturbation of explanation candidates\n",
    "\"\"\"\n",
    "\n",
    "def expand_and_prune(comb, expanded_combis, feature_set, candidates_to_expand, explanations_sets, scores_candidates_to_expand, instance, cf, revert=0, replacements=[]):\n",
    "    \"\"\" Function to expand \"best-first\" feature combination and prune explanation_candidates and candidates_to_expand \"\"\"                \n",
    "    \n",
    "    comb = OrderedSet(comb)\n",
    "    expanded_combis.append(comb)\n",
    "    \n",
    "    old_candidates_to_expand = [frozenset(x) for x in candidates_to_expand]\n",
    "    old_candidates_to_expand = set(old_candidates_to_expand)\n",
    "    \n",
    "    feature_set_new = []\n",
    "    ## If the feature is not in the current combination -> add it to a new list\n",
    "    for feature in feature_set:\n",
    "        if (len(comb & feature) == 0): #set operation: intersection\n",
    "            feature_set_new.append(feature) # If the feature is not in the current combination to remove from the instance\n",
    "    \n",
    "    # Add each element in the new set -> which were initially not present -> to the accepted combination -> create new combinations -> (EXPANSION)\n",
    "    new_explanation_candidates = []\n",
    "    for element in feature_set_new:\n",
    "        union = (comb|element) #set operation: union\n",
    "        new_explanation_candidates.append(union) # Create new combinations to remove from the instance\n",
    "    \n",
    "    #Add new explanation candidates to the list of candidates to expand\n",
    "    candidates_to_expand_notpruned = candidates_to_expand.copy()\n",
    "    for new_candidate in new_explanation_candidates:\n",
    "        candidates_to_expand_notpruned.append(new_candidate)\n",
    "        \n",
    "    # Calculate scores of new combinations and add to scores_candidates_to_expand\n",
    "    # perturb each new candidate and get the score for each.\n",
    "    perturbed_instances = [perturb_fn(x, inst=instance.copy()) for x in new_explanation_candidates]\n",
    "    replacements = [\n",
    "        get_antonyms(feature_names[x[0]], loaded_plain_model_rf.best_estimator_) for x in new_explanation_candidates\n",
    "    ]\n",
    "    replacements = [OrderedSet(x) for x in replacements]\n",
    "    replaced_instances = []\n",
    "    for i in range(len(new_explanation_candidates)):\n",
    "        if replacements[i] == OrderedSet():\n",
    "            replaced_instances.append(perturb_fn(x=new_explanation_candidates[i], inst=instance.copy()))\n",
    "        else:\n",
    "            replaced_instances.append(replace_fn(x=replacements[i], y=replacements[i], inst=instance.copy()))\n",
    "        \n",
    "    perturbed_instances = replaced_instances\n",
    "    scores_perturbed_new = [cf(x, revert) for x in perturbed_instances]\n",
    "    ## Append the newly created score array to the passes existing array\n",
    "    scores_candidates_to_expand_notpruned = scores_candidates_to_expand + scores_perturbed_new\n",
    "    # create a dictionary of scores dictionary where the \n",
    "    # keys are string representations of the candidates from candidates_to_expand_notpruned, and the \n",
    "    # values are the corresponding scores from scores_candidates_to_expand_notpruned\n",
    "    dictionary_scores = dict(zip([str(x) for x in candidates_to_expand_notpruned], scores_candidates_to_expand_notpruned))\n",
    "    \n",
    "    # *** Pruning step: remove all candidates to expand that have an explanation as subset ***\n",
    "    candidates_to_expand_pruned_explanations = []\n",
    "    # take one combination from candidates\n",
    "    for combi in candidates_to_expand_notpruned:\n",
    "        pruning=0\n",
    "        for explanation in explanations_sets: # if an explanation is present as a subser in combi, does not add it to the to be expanded list -> because solution with a smaller size exists\n",
    "            if ((explanation.issubset(combi)) or (explanation==combi)):\n",
    "                pruning = pruning + 1\n",
    "        if (pruning == 0): # If it is not a superset of a present explanation -> add it to the list\n",
    "            candidates_to_expand_pruned_explanations.append(combi)\n",
    "    # Each element is frozen as a set\n",
    "    candidates_to_expand_pruned_explanations_frozen = [frozenset(x) for x in candidates_to_expand_pruned_explanations]\n",
    "    # But the total set f frozen sets are not frozen\n",
    "    candidates_to_expand_pruned_explanations_ = set(candidates_to_expand_pruned_explanations_frozen)\n",
    "    \n",
    "    expanded_combis_frozen = [frozenset(x) for x in expanded_combis]\n",
    "    expanded_combis_ = set(expanded_combis_frozen)\n",
    "        \n",
    "    # *** Pruning step: remove all candidates to expand that are in expanded_combis *** -> Same as above\n",
    "    candidates_to_expand_pruned = (candidates_to_expand_pruned_explanations_ - expanded_combis_)  \n",
    "    ind_dict = dict((k,i) for i,k in enumerate(candidates_to_expand_pruned_explanations_frozen))\n",
    "    indices = [ind_dict[x] for x in candidates_to_expand_pruned]\n",
    "    candidates_to_expand = [candidates_to_expand_pruned_explanations[i] for i in indices]\n",
    "    \n",
    "    #The new explanation candidates are the ones that are NOT in the old list of candidates to expand\n",
    "    new_explanation_candidates_pruned = (candidates_to_expand_pruned - old_candidates_to_expand) \n",
    "    candidates_to_expand_frozen = [frozenset(x) for x in candidates_to_expand]\n",
    "    ind_dict2 = dict((k,i) for i,k in enumerate(candidates_to_expand_frozen))\n",
    "    indices2 = [ind_dict2[x] for x in new_explanation_candidates_pruned]\n",
    "    explanation_candidates = [candidates_to_expand[i] for i in indices2]\n",
    "        \n",
    "    # Get scores of the new candidates and explanations.\n",
    "    scores_candidates_to_expand = [dictionary_scores[x] for x in [str(c) for c in candidates_to_expand]]\n",
    "    scores_explanation_candidates = [dictionary_scores[x] for x in [str(c) for c in explanation_candidates]]\n",
    "    \n",
    "    return (explanation_candidates, candidates_to_expand, expanded_combis, scores_candidates_to_expand, scores_explanation_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdd1eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEDC_Explainer(object):\n",
    "    \"\"\"Class for generating evidence counterfactuals for classifiers on behavioral/text data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_names,\n",
    "        classifier_fn,\n",
    "        threshold_classifier,\n",
    "        max_iter=100,\n",
    "        max_explained=1,\n",
    "        BB=True,\n",
    "        max_features=30,\n",
    "        time_maximum=120,\n",
    "        revert=0,\n",
    "    ):\n",
    "        \"\"\"Init function\n",
    "\n",
    "        Args:\n",
    "            classifier_fn: [function] classifier prediction probability function\n",
    "            or decision function. For ScikitClassifiers, this is classifier.predict_proba\n",
    "            or classifier.decision_function or classifier.predict_log_proba.\n",
    "            Make sure the function only returns one (float) value. For instance, if you\n",
    "            use a ScikitClassifier, transform the classifier.predict_proba as follows:\n",
    "\n",
    "                def classifier_fn(X):\n",
    "                    c=classification_model.predict_proba(X)\n",
    "                    y_predicted_proba=c[:,1]\n",
    "                    return y_predicted_proba\n",
    "\n",
    "            threshold_classifier: [float] the threshold that is used for classifying\n",
    "            instances as positive or not. When score or probability exceeds the\n",
    "            threshold value, then the instance is predicted as positive.\n",
    "            We have no default value, because it is important the user decides\n",
    "            a good value for the threshold.\n",
    "\n",
    "            feature_names: [numpy.array] contains the interpretable feature names,\n",
    "            such as the words themselves in case of document classification or the names\n",
    "            of visited URLs.\n",
    "\n",
    "            max_iter: [int] maximum number of iterations in the search procedure.\n",
    "            Default is set to 50.\n",
    "\n",
    "            max_explained: [int] maximum number of EDC explanations generated.\n",
    "            Default is set to 1.\n",
    "\n",
    "            BB: [“True” or “False”]  when the algorithm is augmented with\n",
    "            branch-and-bound (BB=True), one is only interested in the (set of)\n",
    "            shortest explanation(s). Default is \"True\".\n",
    "\n",
    "            max_features: [int] maximum number of features allowed in the explanation(s).\n",
    "            Default is set to 30.\n",
    "\n",
    "            time_maximum: [int] maximum time allowed to generate explanations,\n",
    "            expressed in minutes. Default is set to 2 minutes (120 seconds).\n",
    "        \"\"\"\n",
    "\n",
    "        self.feature_names = feature_names\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.threshold_classifier = threshold_classifier\n",
    "        self.max_iter = max_iter\n",
    "        self.max_explained = max_explained\n",
    "        self.BB = BB\n",
    "        self.max_features = max_features\n",
    "        self.time_maximum = time_maximum\n",
    "        self.revert = None\n",
    "        self.initial_class = None\n",
    "\n",
    "    def explanation(self, instance):\n",
    "        \"\"\"Generates evidence counterfactual explanation for the instance.\n",
    "        ONLY IF THE CURRENT INSTANCE IS POSITIVE -> Limitation\n",
    "\n",
    "        Args:\n",
    "            instance: [numpy.array or sparse matrix] instance to explain\n",
    "\n",
    "        Returns:\n",
    "            A dictionary where:\n",
    "\n",
    "                explanation_set: explanation(s) ranked from high to low change\n",
    "                in predicted score or probability.\n",
    "                The number of explanations shown depends on the argument max_explained.\n",
    "\n",
    "                number_active_elements: number of active elements of\n",
    "                the instance of interest.\n",
    "\n",
    "                number_explanations: number of explanations found by algorithm.\n",
    "\n",
    "                minimum_size_explanation: number of features in the smallest explanation.\n",
    "\n",
    "                time_elapsed: number of seconds passed to generate explanation(s).\n",
    "\n",
    "                explanations_score_change: change in predicted score/probability\n",
    "                when removing the features in the explanation, ranked from\n",
    "                high to low change.\n",
    "        \"\"\"\n",
    "\n",
    "        # *** INITIALIZATION ***\n",
    "        print(\"Start initialization...\")\n",
    "        tic = time.time()\n",
    "        instance = lil_matrix(instance)\n",
    "        iteration = 0\n",
    "        nb_explanations = 0\n",
    "        minimum_size_explanation = np.nan\n",
    "        explanations = []\n",
    "        explanations_sets = []\n",
    "        explanations_score_change = []\n",
    "        expanded_combis = []\n",
    "        score_predicted = self.classifier_fn(instance)  ## Returns Prediction Prob\n",
    "        # Intial class is 1 is score is greater than threshold\n",
    "        if score_predicted > self.threshold_classifier:\n",
    "            self.initial_class = [1]\n",
    "        else:\n",
    "            self.initial_class = [0]\n",
    "            self.revert = 1\n",
    "        print(\n",
    "            \"score_predicted  \",\n",
    "            score_predicted,\n",
    "            \"  initial_class  \",\n",
    "            self.initial_class,\n",
    "        )\n",
    "\n",
    "        reference = np.reshape(\n",
    "            np.zeros(np.shape(instance)[1]), (1, len(np.zeros(np.shape(instance)[1])))\n",
    "        )\n",
    "        reference = sparse.csr_matrix(reference)\n",
    "\n",
    "        # explainer = shap.KernelExplainer(self.classifier_fn, reference, link=\"identity\")\n",
    "        # shapVals = explainer.shap_values(instance, nsamples=5000, l1_reg=\"aic\")\n",
    "\n",
    "        # features = []\n",
    "        # for ind in range(len(shapVals[0])):\n",
    "        #     if shapVals[0, ind] != 0:\n",
    "        #         features.append({\"feature\": ind, \"shapValue\": shapVals[0, ind]})\n",
    "        # sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"], reverse=True)\n",
    "        # inverse_sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"])\n",
    "\n",
    "        # if self.revert == 1:\n",
    "        #     sorted_data_in = inverse_sorted_data_in\n",
    "\n",
    "        indices_active_elements = np.nonzero(instance)[\n",
    "            1\n",
    "        ]  ## -> Gets non zero elements in the instance as an array [x, y, z]\n",
    "        # sorted_indices = sorted(\n",
    "        #     indices_active_elements, key=lambda x: shapVals[0, x], reverse=True\n",
    "        # )\n",
    "        # indices_active_elements = np.array(sorted_indices)\n",
    "        number_active_elements = len(indices_active_elements)\n",
    "        indices_active_elements = indices_active_elements.reshape(\n",
    "            (number_active_elements, 1)\n",
    "        )  ## -> Reshape to get a predictable\n",
    "\n",
    "        candidates_to_expand = (\n",
    "            []\n",
    "        )  # -> These combinations are further expanded -> These are the elements to be removed from the sentence\n",
    "        for features in indices_active_elements:\n",
    "            candidates_to_expand.append(OrderedSet(features))\n",
    "        print(\"candidates_to_expand \", candidates_to_expand)\n",
    "        ## > Gets an array with each element in reshaped incides as an ordered set -> [OrderedSet([430]), OrderedSet([588]), OrderedSet([595])]\n",
    "\n",
    "        explanation_candidates = candidates_to_expand.copy()\n",
    "        print(\"explanation_candidates \", explanation_candidates)\n",
    "        ## Gets a copy of the above array -> Initially\n",
    "\n",
    "        feature_set = [\n",
    "            frozenset(x) for x in indices_active_elements\n",
    "        ]  ## Immutable -> can be used as keys in dictionary\n",
    "        ## Used features in the current x-reference -> incides of the words in the review.\n",
    "\n",
    "        print(\"Initialization is complete.\")\n",
    "        print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** WHILE LOOP ***\n",
    "        while (\n",
    "            (iteration < self.max_iter)\n",
    "            and (nb_explanations < self.max_explained)\n",
    "            and (len(candidates_to_expand) != 0)\n",
    "            and (len(explanation_candidates) != 0)\n",
    "            and ((time.time() - tic) < self.time_maximum)\n",
    "        ):\n",
    "            ## Stop if maximum iterations exceeded\n",
    "            #  number of explanations generated is greater than the maximum explanations\n",
    "            #  There are no candidates to expand\n",
    "            #  There are no explanation candidates -> Used to force stop while loop below\n",
    "            #  Or maximum allowed time exceeded\n",
    "            iteration += 1\n",
    "            print(\"\\n Iteration %d \\n\" % iteration)\n",
    "\n",
    "            if iteration == 1:\n",
    "                print(\"Run in first iteration -> perturbation done \\n\")\n",
    "                # Print the word in each index in the explanation candidates\n",
    "                # for item in explanation_candidates:\n",
    "                #     print([self.feature_names[x] for x in item])\n",
    "                replacements = [\n",
    "                    get_antonyms(\n",
    "                        self.feature_names[x[0]], loaded_plain_model_rf.best_estimator_\n",
    "                    )\n",
    "                    for x in explanation_candidates\n",
    "                ]\n",
    "                # convert each element in replacement to a OrderedSet\n",
    "                replacements = [OrderedSet(x) for x in replacements]\n",
    "                print(\"replacements \\n\", replacements, \"\\n\")\n",
    "                print(\"explanation_candidates \\n\", explanation_candidates, \"\\n\")\n",
    "                perturbed_instances = [\n",
    "                    perturb_fn(x, inst=instance.copy()) for x in explanation_candidates\n",
    "                ]\n",
    "                replaced_instances = []\n",
    "                for i in range(len(explanation_candidates)):\n",
    "                    if replacements[i] == OrderedSet():\n",
    "                        replaced_instances.append(\n",
    "                            perturb_fn(\n",
    "                                x=explanation_candidates[i], inst=instance.copy()\n",
    "                            )\n",
    "                        )\n",
    "                    else:\n",
    "                        replaced_instances.append(\n",
    "                            replace_fn(\n",
    "                                x=explanation_candidates[i],\n",
    "                                y=replacements[i],\n",
    "                                inst=instance.copy(),\n",
    "                            )\n",
    "                        )\n",
    "                print(\"replaced_instances \\n\", replaced_instances, \"\\n\")\n",
    "                # Remove the elements in the indices given by the ordered set x and return an array fo such elements\n",
    "                # Removes only one element in the first run -> Contains sentences with one word removed\n",
    "                perturbed_instances = replaced_instances\n",
    "                scores_explanation_candidates = [\n",
    "                    self.classifier_fn(x, self.revert) for x in perturbed_instances\n",
    "                ]\n",
    "                # Get predictions for each perturbed instance where one or more elements are removed from the initial instance\n",
    "                # It is in form of [[x], [y], [z]]\n",
    "                print(\n",
    "                    \"scores_explanation_candidates \\n\",\n",
    "                    scores_explanation_candidates,\n",
    "                    \"\\n\",\n",
    "                )\n",
    "                scores_candidates_to_expand = scores_explanation_candidates.copy()\n",
    "\n",
    "            scores_perturbed_new_combinations = [\n",
    "                x[0] for x in scores_explanation_candidates\n",
    "            ]\n",
    "            # Therefore get it to the shape [x, y, z] by getting the [0] th element of each element array\n",
    "            # print(\n",
    "            #     \"scores_perturbed_new_combinations \", scores_perturbed_new_combinations\n",
    "            # )\n",
    "\n",
    "            # ***CHECK IF THERE ARE EXPLANATIONS***\n",
    "            new_explanations = list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # Get explanation candidates where their probability is less than the threshold classifier -> Positive becomes negative\n",
    "            # print(\"New Explanations \\n\", new_explanations)\n",
    "            explanations += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # print(\"\\n explanations, explanations_score_change\", explanations)\n",
    "            nb_explanations += len(\n",
    "                list(\n",
    "                    compress(\n",
    "                        explanation_candidates,\n",
    "                        scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                    )\n",
    "                )\n",
    "            )  # Update number of explanations which pass the required threshold\n",
    "            explanations_sets += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            explanations_sets = [\n",
    "                set(x) for x in explanations_sets\n",
    "            ]  # Convert each array to a set -> to get the words\n",
    "            explanations_score_change += list(\n",
    "                compress(\n",
    "                    scores_explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # print('explanations_score_change', explanations_score_change)\n",
    "\n",
    "            # Adjust max_length\n",
    "            if self.BB == True:\n",
    "                if len(explanations) != 0:\n",
    "                    lengths = []  # Record length of each explanation found\n",
    "                    for explanation in explanations:\n",
    "                        lengths.append(len(explanation))\n",
    "                    lengths = np.array(lengths)\n",
    "                    max_length = lengths.min()\n",
    "                    # Get minimum length of the found explanations as max length -> Do not search for explanations with longer length\n",
    "                else:\n",
    "                    max_length = number_active_elements  # Else can find maximum length equal to number of words in instance\n",
    "            else:\n",
    "                max_length = number_active_elements\n",
    "            print(\"\\n-------------Max length updated to - \", max_length)\n",
    "\n",
    "            # Eliminate combinations from candidates_to_expand (\"best-first\" candidates) that can not be expanded\n",
    "            # Pruning based on Branch & Bound=True, max. features allowed and number of active features\n",
    "            candidates_to_expand_updated = []\n",
    "            scores_candidates_to_expand_updated = (\n",
    "                []\n",
    "            )  # enumerate -> Find count of || to list one after another\n",
    "            for j, combination in enumerate(candidates_to_expand):\n",
    "                if (\n",
    "                    (len(combination) < number_active_elements)\n",
    "                    and (len(combination) < max_length)\n",
    "                    and (len(combination) < self.max_features)\n",
    "                ):\n",
    "                    # Combination length should be less than the words in the input and max length of the required explanation and required maximum features\n",
    "                    candidates_to_expand_updated.append(\n",
    "                        combination\n",
    "                    )  # If the combination matches, it is further expanded\n",
    "                    scores_candidates_to_expand_updated.append(\n",
    "                        scores_candidates_to_expand[j]\n",
    "                    )\n",
    "                    # Add the prediction score to the new array\n",
    "                    # get the score from the scores_candidates_to_expand using the current index\n",
    "\n",
    "            print(\n",
    "                \"\\nlen(candidates_to_expand_updated)\",\n",
    "                len(candidates_to_expand_updated),\n",
    "                \" 0 \",\n",
    "            )\n",
    "            print(\n",
    "                \"\\nnb_explanations\",\n",
    "                nb_explanations,\n",
    "                \" >= self.max_explained \",\n",
    "                self.max_explained,\n",
    "            )\n",
    "\n",
    "            # *** IF LOOP ***\n",
    "            # expanding the candidates to update will exceed the max length set in the earlier loop\n",
    "            if (len(candidates_to_expand_updated) == 0) or (\n",
    "                nb_explanations >= self.max_explained\n",
    "            ):\n",
    "                ## If the number of explanations exceeded the required number\n",
    "                ## or no candidates\n",
    "                ## no explanations present\n",
    "\n",
    "                print(\"nb_explanations Stop iterations...\")\n",
    "                explanation_candidates = []  # stop algorithm\n",
    "                ## Found all the candidates\n",
    "                print(\n",
    "                    \"scores_candidates_to_expand_updated  \",\n",
    "                    scores_candidates_to_expand_updated,\n",
    "                )\n",
    "                # print(\"candidates_to_expand_updated   \", candidates_to_expand_updated)\n",
    "\n",
    "            elif len(candidates_to_expand_updated) != 0:\n",
    "                ## If there are possible candidates\n",
    "\n",
    "                explanation_candidates = []\n",
    "                it = 0  # Iteration of the while loop\n",
    "                indices = []\n",
    "\n",
    "                scores_candidates_to_expand2 = []\n",
    "                for score in scores_candidates_to_expand_updated:\n",
    "                    if score[0] < self.threshold_classifier:\n",
    "                        scores_candidates_to_expand2.append(2 * score_predicted)\n",
    "                    else:\n",
    "                        scores_candidates_to_expand2.append(score)\n",
    "                # update candidate scores if they have score less than threshold -> To expand them further\n",
    "                # shap_candidates_to_expand2 = []\n",
    "                # for candidate in candidates_to_expand_updated:\n",
    "                #     shapValues = 0\n",
    "                #     for word in candidate:\n",
    "                #         # find word in feature column in sorted_data\n",
    "                #         for ind in range(len(sorted_data_in)):\n",
    "                #             if sorted_data_in[ind][\"feature\"] == word:\n",
    "                #                 shapValues += sorted_data_in[ind][\"shapValue\"]\n",
    "                #                 break\n",
    "                #     shap_candidates_to_expand2.append(shapValues)\n",
    "\n",
    "                # print(\n",
    "                #     \"\\n scores_candidates_to_expand2 before loop\",\n",
    "                #     scores_candidates_to_expand2,\n",
    "                # )\n",
    "\n",
    "                # *** WHILE LOOP ***\n",
    "                while (\n",
    "                    (len(explanation_candidates) == 0)\n",
    "                    and (it < len(scores_candidates_to_expand2))\n",
    "                    and ((time.time() - tic) < self.time_maximum)\n",
    "                ):\n",
    "                    # Stop if candidates are found or looped through more than there are candidates or maximum time reached\n",
    "\n",
    "                    print(\"While loop iteration %d\" % it)\n",
    "\n",
    "                    if it != 0:  # Because indices are not there in the first iteration\n",
    "                        for index in indices:\n",
    "                            scores_candidates_to_expand2[index] = 2 * score_predicted\n",
    "\n",
    "                    # print(\n",
    "                    #     \"\\n scores_candidates_to_expand2 after loop\",\n",
    "                    #     scores_candidates_to_expand2,\n",
    "                    # )\n",
    "                    # print(\"\\n indices\", indices)\n",
    "\n",
    "                    # do elementwise subtraction between score_predicted and scores_candidates_to_expand2\n",
    "                    subtractionList = []\n",
    "                    for x, y in zip(score_predicted, scores_candidates_to_expand2):\n",
    "                        print(\"\\n x, y\", x - y)\n",
    "                        subtractionList.append(x - y)\n",
    "\n",
    "                    # Do element wise subtraction between the prediction score of the x_ref and every element of the scores_candidates_to_expand2\n",
    "                    index_combi_max = np.argmax(subtractionList)\n",
    "                    # index_shap_max = np.argmax(shap_candidates_to_expand2)\n",
    "                    # index_shap_min = np.argmin(shap_candidates_to_expand2)\n",
    "                    if self.revert == 0:\n",
    "                        index_combi_max = np.argmax(subtractionList)\n",
    "                    else:\n",
    "                        index_combi_max = np.argmin(subtractionList)\n",
    "                    # if self.revert == 0:\n",
    "                    #     index_combi_max = index_shap_max\n",
    "                    # else:\n",
    "                    #     index_combi_max = index_shap_min\n",
    "                    # print(\n",
    "                    #     \"subtrac max \",\n",
    "                    #     index_combi_max,\n",
    "                    #     \" index_shap_max \",\n",
    "                    #     index_shap_max,\n",
    "                    # )\n",
    "                    # Get the index of the maximum value -> Expand it\n",
    "                    print(\n",
    "                        \"\\n index_combi_max\",\n",
    "                        candidates_to_expand_updated[np.argmax(subtractionList)],\n",
    "                        \"\\n index_shap_max\",\n",
    "                        candidates_to_expand_updated[index_combi_max],\n",
    "                    )\n",
    "                    indices.append(index_combi_max)\n",
    "                    expanded_combis.append(\n",
    "                        candidates_to_expand_updated[index_combi_max]\n",
    "                    )\n",
    "                    # Add this combination to already expanded combinations as it will be expanded next by expand and prune function\n",
    "\n",
    "                    comb_to_expand = candidates_to_expand_updated[index_combi_max]\n",
    "                    # Expand the found combination with highest difference\n",
    "                    func = expand_and_prune(\n",
    "                        comb_to_expand,\n",
    "                        expanded_combis,\n",
    "                        feature_set,\n",
    "                        candidates_to_expand_updated,\n",
    "                        explanations_sets,\n",
    "                        scores_candidates_to_expand_updated,\n",
    "                        instance,\n",
    "                        self.classifier_fn,\n",
    "                        self.revert,\n",
    "                        replacements,\n",
    "                    )\n",
    "                    \"\"\"Returns:\n",
    "                        - explanation_candidates: combinations of features that are explanation\n",
    "                        candidates to be checked in the next iteration\n",
    "                        - candidates_to_expand: combinations of features that are candidates to\n",
    "                        expanded in next iterations or candidates for \"best-first\"\n",
    "                        - expanded_combis: [list] list of combinations of features that are already\n",
    "                        expanded as \"best-first\"\n",
    "                        - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "                        combinations of features to be expanded\n",
    "                        - scores_explanation_candidates: scores after perturbation of explanation candidates\"\"\"\n",
    "                    explanation_candidates = func[0]\n",
    "                    candidates_to_expand = func[1]\n",
    "                    expanded_combis = func[2]\n",
    "                    scores_candidates_to_expand = func[3]\n",
    "                    scores_explanation_candidates = func[4]\n",
    "\n",
    "                    it += 1\n",
    "\n",
    "                print(\n",
    "                    \"\\n\\n\\niteration - \", iteration, \" self.max_iter - \", self.max_iter\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(candidates_to_expand) - \",\n",
    "                    len(candidates_to_expand),\n",
    "                    \" != 0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(explanation_candidates) - \",\n",
    "                    len(explanation_candidates),\n",
    "                    \" !=0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\n(time.time() - tic) - \",\n",
    "                    (time.time() - tic),\n",
    "                    \" self.time_maximum - \",\n",
    "                    self.time_maximum,\n",
    "                )\n",
    "            print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** FINAL PART OF ALGORITHM ***\n",
    "        print(\"Iterations are done.\")\n",
    "\n",
    "        explanation_set = []\n",
    "        explanation_feature_names = []\n",
    "        for i in range(len(explanations)):\n",
    "            explanation_feature_names = []\n",
    "            for features in explanations[i]:\n",
    "                explanation_feature_names.append(self.feature_names[features])\n",
    "            explanation_set.append(explanation_feature_names)\n",
    "\n",
    "        if len(explanations) != 0:\n",
    "            lengths_explanation = []\n",
    "            for explanation in explanations:\n",
    "                l = len(explanation)\n",
    "                lengths_explanation.append(l)\n",
    "            minimum_size_explanation = np.min(lengths_explanation)\n",
    "            index_of_min_length_explanation = np.argmin(lengths_explanation)\n",
    "        try:\n",
    "            print(\"argmin\", explanations[index_of_min_length_explanation])\n",
    "        except:\n",
    "            pass\n",
    "        new_instance = instance.copy()\n",
    "        new_replacements = []\n",
    "        replacement_features = []\n",
    "        for feature in explanations[index_of_min_length_explanation]:\n",
    "            feature_replacement = get_antonyms(\n",
    "                feature_names[feature], loaded_plain_model_rf.best_estimator_\n",
    "            )\n",
    "            print(\"feature_replacement\", feature_replacement)\n",
    "            new_replacements.append(feature_replacement)\n",
    "        print(\"new_replacements\", new_replacements)\n",
    "        print(\"replacementfeature\", feature_names[new_replacements[0]])\n",
    "        try:\n",
    "            replacementWords = []\n",
    "            for item_ind in range(len(new_replacements)):\n",
    "                replacementWords.append(\n",
    "                    {\n",
    "                        \"feature\": feature_names[\n",
    "                            explanations[index_of_min_length_explanation][item_ind]\n",
    "                        ],\n",
    "                        \"replacement\": feature_names[new_replacements[item_ind]][0],\n",
    "                    }\n",
    "                )\n",
    "            print(\"replacementWords\", replacementWords)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        new_insatnce = instance.copy()\n",
    "        for relpacement_feature_index in range(len(new_replacements)):\n",
    "            if new_replacements[relpacement_feature_index] != []:\n",
    "                new_insatnce = replace_fn(\n",
    "                    x=explanations[index_of_min_length_explanation][\n",
    "                        relpacement_feature_index\n",
    "                    ],\n",
    "                    y=new_replacements[relpacement_feature_index],\n",
    "                    inst=new_insatnce,\n",
    "                )\n",
    "                replacement_features.append(\n",
    "                    feature_names[new_replacements[relpacement_feature_index]]\n",
    "                )\n",
    "            else:\n",
    "                new_insatnce = perturb_fn(\n",
    "                    explanations[index_of_min_length_explanation][\n",
    "                        relpacement_feature_index\n",
    "                    ],\n",
    "                    new_insatnce,\n",
    "                )\n",
    "                replacement_features.append(feature_names[relpacement_feature_index])\n",
    "        final_prob = self.classifier_fn(new_insatnce)\n",
    "        print(\"final_prob\", final_prob)\n",
    "\n",
    "        number_explanations = len(explanations)\n",
    "        if np.size(explanations_score_change) > 1:\n",
    "            inds = np.argsort(explanations_score_change, axis=0)\n",
    "            inds = np.fliplr([inds])[0]\n",
    "            inds_2 = []\n",
    "            for i in range(np.size(inds)):\n",
    "                inds_2.append(inds[i][0])\n",
    "            explanation_set_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanation_set_adjusted.append(explanation_set[j])\n",
    "            explanations_score_change_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanations_score_change_adjusted.append(explanations_score_change[j])\n",
    "            explanation_set = explanation_set_adjusted\n",
    "            explanations_score_change = explanations_score_change_adjusted\n",
    "\n",
    "        time_elapsed = time.time() - tic\n",
    "        print(\"\\n Total elapsed time %d \\n\" % time_elapsed)\n",
    "\n",
    "        print(\n",
    "            \"If we change the words \",\n",
    "            explanation_set[0 : self.max_explained],\n",
    "            \"From the review, the prediction will be reversed\",\n",
    "        )\n",
    "        indices_active_elements = np.nonzero(instance)[1]\n",
    "        # Find the elements in indices_active_elements_explain that are not in indices_active_elements\n",
    "        print(\"indices_active_elements\", indices_active_elements)\n",
    "\n",
    "        return {\n",
    "            \"explanation set\": explanation_set[0 : self.max_explained],\n",
    "            \"number active elements\": number_active_elements,\n",
    "            \"number explanations found\": number_explanations,\n",
    "            \"size smallest explanation\": minimum_size_explanation,\n",
    "            \"time elapsed\": time_elapsed,\n",
    "            \"differences score\": explanations_score_change[0 : self.max_explained],\n",
    "            \"iterations\": iteration,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5bb9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEDC_Explainer_no(object):\n",
    "    \"\"\"Class for generating evidence counterfactuals for classifiers on behavioral/text data\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_names,\n",
    "        classifier_fn,\n",
    "        threshold_classifier,\n",
    "        max_iter=100,\n",
    "        max_explained=1,\n",
    "        BB=True,\n",
    "        max_features=30,\n",
    "        time_maximum=120,\n",
    "        revert=0,\n",
    "    ):\n",
    "        \"\"\"Init function\n",
    "\n",
    "        Args:\n",
    "            classifier_fn: [function] classifier prediction probability function\n",
    "            or decision function. For ScikitClassifiers, this is classifier.predict_proba\n",
    "            or classifier.decision_function or classifier.predict_log_proba.\n",
    "            Make sure the function only returns one (float) value. For instance, if you\n",
    "            use a ScikitClassifier, transform the classifier.predict_proba as follows:\n",
    "\n",
    "                def classifier_fn(X):\n",
    "                    c=classification_model.predict_proba(X)\n",
    "                    y_predicted_proba=c[:,1]\n",
    "                    return y_predicted_proba\n",
    "\n",
    "            threshold_classifier: [float] the threshold that is used for classifying\n",
    "            instances as positive or not. When score or probability exceeds the\n",
    "            threshold value, then the instance is predicted as positive.\n",
    "            We have no default value, because it is important the user decides\n",
    "            a good value for the threshold.\n",
    "\n",
    "            feature_names: [numpy.array] contains the interpretable feature names,\n",
    "            such as the words themselves in case of document classification or the names\n",
    "            of visited URLs.\n",
    "\n",
    "            max_iter: [int] maximum number of iterations in the search procedure.\n",
    "            Default is set to 50.\n",
    "\n",
    "            max_explained: [int] maximum number of EDC explanations generated.\n",
    "            Default is set to 1.\n",
    "\n",
    "            BB: [“True” or “False”]  when the algorithm is augmented with\n",
    "            branch-and-bound (BB=True), one is only interested in the (set of)\n",
    "            shortest explanation(s). Default is \"True\".\n",
    "\n",
    "            max_features: [int] maximum number of features allowed in the explanation(s).\n",
    "            Default is set to 30.\n",
    "\n",
    "            time_maximum: [int] maximum time allowed to generate explanations,\n",
    "            expressed in minutes. Default is set to 2 minutes (120 seconds).\n",
    "        \"\"\"\n",
    "\n",
    "        self.feature_names = feature_names\n",
    "        self.classifier_fn = classifier_fn\n",
    "        self.threshold_classifier = threshold_classifier\n",
    "        self.max_iter = max_iter\n",
    "        self.max_explained = max_explained\n",
    "        self.BB = BB\n",
    "        self.max_features = max_features\n",
    "        self.time_maximum = time_maximum\n",
    "        self.revert = None\n",
    "        self.initial_class = None\n",
    "\n",
    "    def explanation(self, instance):\n",
    "        \"\"\"Generates evidence counterfactual explanation for the instance.\n",
    "        ONLY IF THE CURRENT INSTANCE IS POSITIVE -> Limitation\n",
    "\n",
    "        Args:\n",
    "            instance: [numpy.array or sparse matrix] instance to explain\n",
    "\n",
    "        Returns:\n",
    "            A dictionary where:\n",
    "\n",
    "                explanation_set: explanation(s) ranked from high to low change\n",
    "                in predicted score or probability.\n",
    "                The number of explanations shown depends on the argument max_explained.\n",
    "\n",
    "                number_active_elements: number of active elements of\n",
    "                the instance of interest.\n",
    "\n",
    "                number_explanations: number of explanations found by algorithm.\n",
    "\n",
    "                minimum_size_explanation: number of features in the smallest explanation.\n",
    "\n",
    "                time_elapsed: number of seconds passed to generate explanation(s).\n",
    "\n",
    "                explanations_score_change: change in predicted score/probability\n",
    "                when removing the features in the explanation, ranked from\n",
    "                high to low change.\n",
    "        \"\"\"\n",
    "\n",
    "        # *** INITIALIZATION ***\n",
    "        print(\"Start initialization...\")\n",
    "        tic = time.time()\n",
    "        instance = lil_matrix(instance)\n",
    "        iteration = 0\n",
    "        nb_explanations = 0\n",
    "        minimum_size_explanation = np.nan\n",
    "        explanations = []\n",
    "        explanations_sets = []\n",
    "        explanations_score_change = []\n",
    "        expanded_combis = []\n",
    "        score_predicted = self.classifier_fn(instance)  ## Returns Prediction Prob\n",
    "        # Intial class is 1 is score is greater than threshold\n",
    "        if score_predicted > self.threshold_classifier:\n",
    "            self.initial_class = [1]\n",
    "        else:\n",
    "            self.initial_class = [0]\n",
    "            self.revert = 1\n",
    "        print(\"score_predicted  \", score_predicted, \"  initial_class  \", self.initial_class)\n",
    "\n",
    "        reference = np.reshape(\n",
    "            np.zeros(np.shape(instance)[1]), (1, len(np.zeros(np.shape(instance)[1])))\n",
    "        )\n",
    "        reference = sparse.csr_matrix(reference)\n",
    "\n",
    "        explainer = shap.KernelExplainer(self.classifier_fn, reference, link=\"identity\")\n",
    "        shapVals = explainer.shap_values(instance, nsamples=5000, l1_reg=\"aic\")\n",
    "\n",
    "        features = []\n",
    "        for ind in range(len(shapVals[0])):\n",
    "            if shapVals[0, ind] != 0:\n",
    "                features.append({\"feature\": ind, \"shapValue\": shapVals[0, ind]})\n",
    "        sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"], reverse=True)\n",
    "        inverse_sorted_data_in = sorted(features, key=lambda x: x[\"shapValue\"])\n",
    "\n",
    "        if self.revert == 1:\n",
    "            sorted_data_in = inverse_sorted_data_in\n",
    "\n",
    "        indices_active_elements = np.nonzero(instance)[\n",
    "            1\n",
    "        ]  ## -> Gets non zero elements in the instance as an array [x, y, z]\n",
    "        sorted_indices = sorted(\n",
    "            indices_active_elements, key=lambda x: shapVals[0, x], reverse=True\n",
    "        )\n",
    "        indices_active_elements = np.array(sorted_indices)\n",
    "        number_active_elements = len(indices_active_elements)\n",
    "        indices_active_elements = indices_active_elements.reshape(\n",
    "            (number_active_elements, 1)\n",
    "        )  ## -> Reshape to get a predictable\n",
    "\n",
    "        candidates_to_expand = (\n",
    "            []\n",
    "        )  # -> These combinations are further expanded -> These are the elements to be removed from the sentence\n",
    "        for features in indices_active_elements:\n",
    "            candidates_to_expand.append(OrderedSet(features))\n",
    "        print(\"candidates_to_expand \", candidates_to_expand)\n",
    "        ## > Gets an array with each element in reshaped incides as an ordered set -> [OrderedSet([430]), OrderedSet([588]), OrderedSet([595])]\n",
    "\n",
    "        explanation_candidates = candidates_to_expand.copy()\n",
    "        print(\"explanation_candidates \", explanation_candidates)\n",
    "        ## Gets a copy of the above array -> Initially\n",
    "\n",
    "        feature_set = [\n",
    "            frozenset(x) for x in indices_active_elements\n",
    "        ]  ## Immutable -> can be used as keys in dictionary\n",
    "        ## Used features in the current x-reference -> incides of the words in the review.\n",
    "\n",
    "        print(\"Initialization is complete.\")\n",
    "        print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** WHILE LOOP ***\n",
    "        while (\n",
    "            (iteration < self.max_iter)\n",
    "            and (nb_explanations < self.max_explained)\n",
    "            and (len(candidates_to_expand) != 0)\n",
    "            and (len(explanation_candidates) != 0)\n",
    "            and ((time.time() - tic) < self.time_maximum)\n",
    "        ):\n",
    "            ## Stop if maximum iterations exceeded\n",
    "            #  number of explanations generated is greater than the maximum explanations\n",
    "            #  There are no candidates to expand\n",
    "            #  There are no explanation candidates -> Used to force stop while loop below\n",
    "            #  Or maximum allowed time exceeded\n",
    "            iteration += 1\n",
    "            print(\"\\n Iteration %d \\n\" % iteration)\n",
    "\n",
    "            if iteration == 1:\n",
    "                print(\"Run in first iteration -> perturbation done \\n\")\n",
    "                # Print the word in each index in the explanation candidates\n",
    "                # for item in explanation_candidates:\n",
    "                #     print([self.feature_names[x] for x in item])\n",
    "                replacements = [\n",
    "                    get_antonyms(\n",
    "                        self.feature_names[x[0]], loaded_plain_model_rf.best_estimator_\n",
    "                    )\n",
    "                    for x in explanation_candidates\n",
    "                ]\n",
    "                # convert each element in replacement to a OrderedSet\n",
    "                replacements = [OrderedSet(x) for x in replacements]\n",
    "                print(\"replacements \\n\", replacements, \"\\n\")\n",
    "                print(\"explanation_candidates \\n\", explanation_candidates, \"\\n\")\n",
    "                perturbed_instances = [\n",
    "                    perturb_fn(x, inst=instance.copy()) for x in explanation_candidates\n",
    "                ]\n",
    "                replaced_instances = []\n",
    "                for i in range(len(explanation_candidates)):\n",
    "                    if replacements[i] == OrderedSet():\n",
    "                        replaced_instances.append(\n",
    "                            perturb_fn(\n",
    "                                x=explanation_candidates[i], inst=instance.copy()\n",
    "                            )\n",
    "                        )\n",
    "                    else:\n",
    "                        replaced_instances.append(\n",
    "                            replace_fn(\n",
    "                                x=explanation_candidates[i],\n",
    "                                y=replacements[i],\n",
    "                                inst=instance.copy(),\n",
    "                            )\n",
    "                        )\n",
    "                print('replaced_instances \\n', replaced_instances, '\\n')\n",
    "                # Remove the elements in the indices given by the ordered set x and return an array fo such elements\n",
    "                # Removes only one element in the first run -> Contains sentences with one word removed\n",
    "                perturbed_instances = replaced_instances\n",
    "                scores_explanation_candidates = [\n",
    "                    self.classifier_fn(x, self.revert) for x in perturbed_instances\n",
    "                ]\n",
    "                # Get predictions for each perturbed instance where one or more elements are removed from the initial instance\n",
    "                # It is in form of [[x], [y], [z]]\n",
    "                print(\n",
    "                    \"scores_explanation_candidates \\n\",\n",
    "                    scores_explanation_candidates,\n",
    "                    \"\\n\",\n",
    "                )\n",
    "                scores_candidates_to_expand = scores_explanation_candidates.copy()\n",
    "\n",
    "            scores_perturbed_new_combinations = [\n",
    "                x[0] for x in scores_explanation_candidates\n",
    "            ]\n",
    "            # Therefore get it to the shape [x, y, z] by getting the [0] th element of each element array\n",
    "            # print(\n",
    "            #     \"scores_perturbed_new_combinations \", scores_perturbed_new_combinations\n",
    "            # )\n",
    "\n",
    "            # ***CHECK IF THERE ARE EXPLANATIONS***\n",
    "            new_explanations = list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # Get explanation candidates where their probability is less than the threshold classifier -> Positive becomes negative\n",
    "            # print(\"New Explanations \\n\", new_explanations)\n",
    "            explanations += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # print(\"\\n explanations, explanations_score_change\", explanations)\n",
    "            nb_explanations += len(\n",
    "                list(\n",
    "                    compress(\n",
    "                        explanation_candidates,\n",
    "                        scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                    )\n",
    "                )\n",
    "            )  # Update number of explanations which pass the required threshold\n",
    "            explanations_sets += list(\n",
    "                compress(\n",
    "                    explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            explanations_sets = [\n",
    "                set(x) for x in explanations_sets\n",
    "            ]  # Convert each array to a set -> to get the words\n",
    "            explanations_score_change += list(\n",
    "                compress(\n",
    "                    scores_explanation_candidates,\n",
    "                    scores_perturbed_new_combinations < self.threshold_classifier,\n",
    "                )\n",
    "            )\n",
    "            # print('explanations_score_change', explanations_score_change)\n",
    "\n",
    "            # Adjust max_length\n",
    "            if self.BB == True:\n",
    "                if len(explanations) != 0:\n",
    "                    lengths = []  # Record length of each explanation found\n",
    "                    for explanation in explanations:\n",
    "                        lengths.append(len(explanation))\n",
    "                    lengths = np.array(lengths)\n",
    "                    max_length = lengths.min()\n",
    "                    # Get minimum length of the found explanations as max length -> Do not search for explanations with longer length\n",
    "                else:\n",
    "                    max_length = number_active_elements  # Else can find maximum length equal to number of words in instance\n",
    "            else:\n",
    "                max_length = number_active_elements\n",
    "            print(\"\\n-------------Max length updated to - \", max_length)\n",
    "\n",
    "            # Eliminate combinations from candidates_to_expand (\"best-first\" candidates) that can not be expanded\n",
    "            # Pruning based on Branch & Bound=True, max. features allowed and number of active features\n",
    "            candidates_to_expand_updated = []\n",
    "            scores_candidates_to_expand_updated = (\n",
    "                []\n",
    "            )  # enumerate -> Find count of || to list one after another\n",
    "            for j, combination in enumerate(candidates_to_expand):\n",
    "                if (\n",
    "                    (len(combination) < number_active_elements)\n",
    "                    and (len(combination) < max_length)\n",
    "                    and (len(combination) < self.max_features)\n",
    "                ):\n",
    "                    # Combination length should be less than the words in the input and max length of the required explanation and required maximum features\n",
    "                    candidates_to_expand_updated.append(\n",
    "                        combination\n",
    "                    )  # If the combination matches, it is further expanded\n",
    "                    scores_candidates_to_expand_updated.append(\n",
    "                        scores_candidates_to_expand[j]\n",
    "                    )\n",
    "                    # Add the prediction score to the new array\n",
    "                    # get the score from the scores_candidates_to_expand using the current index\n",
    "\n",
    "            print(\n",
    "                \"\\nlen(candidates_to_expand_updated)\",\n",
    "                len(candidates_to_expand_updated),\n",
    "                \" 0 \",\n",
    "            )\n",
    "            print(\n",
    "                \"\\nnb_explanations\",\n",
    "                nb_explanations,\n",
    "                \" >= self.max_explained \",\n",
    "                self.max_explained,\n",
    "            )\n",
    "\n",
    "            # *** IF LOOP ***\n",
    "            # expanding the candidates to update will exceed the max length set in the earlier loop\n",
    "            if (len(candidates_to_expand_updated) == 0) or (\n",
    "                nb_explanations >= self.max_explained\n",
    "            ):\n",
    "                ## If the number of explanations exceeded the required number\n",
    "                ## or no candidates\n",
    "                ## no explanations present\n",
    "\n",
    "                print(\"nb_explanations Stop iterations...\")\n",
    "                explanation_candidates = []  # stop algorithm\n",
    "                ## Found all the candidates\n",
    "                print(\n",
    "                    \"scores_candidates_to_expand_updated  \",\n",
    "                    scores_candidates_to_expand_updated,\n",
    "                )\n",
    "                # print(\"candidates_to_expand_updated   \", candidates_to_expand_updated)\n",
    "\n",
    "            elif len(candidates_to_expand_updated) != 0:\n",
    "                ## If there are possible candidates\n",
    "\n",
    "                explanation_candidates = []\n",
    "                it = 0  # Iteration of the while loop\n",
    "                indices = []\n",
    "\n",
    "                scores_candidates_to_expand2 = []\n",
    "                for score in scores_candidates_to_expand_updated:\n",
    "                    if score[0] < self.threshold_classifier:\n",
    "                        scores_candidates_to_expand2.append(2 * score_predicted)\n",
    "                    else:\n",
    "                        scores_candidates_to_expand2.append(score)\n",
    "                # update candidate scores if they have score less than threshold -> To expand them further\n",
    "                shap_candidates_to_expand2 = []\n",
    "                for candidate in candidates_to_expand_updated:\n",
    "                    shapValues = 0\n",
    "                    for word in candidate:\n",
    "                        # find word in feature column in sorted_data\n",
    "                        for ind in range(len(sorted_data_in)):\n",
    "                            if sorted_data_in[ind][\"feature\"] == word:\n",
    "                                shapValues += sorted_data_in[ind][\"shapValue\"]\n",
    "                                break\n",
    "                    shap_candidates_to_expand2.append(shapValues)\n",
    "\n",
    "                # print(\n",
    "                #     \"\\n scores_candidates_to_expand2 before loop\",\n",
    "                #     scores_candidates_to_expand2,\n",
    "                # )\n",
    "\n",
    "                # *** WHILE LOOP ***\n",
    "                while (\n",
    "                    (len(explanation_candidates) == 0)\n",
    "                    and (it < len(scores_candidates_to_expand2))\n",
    "                    and ((time.time() - tic) < self.time_maximum)\n",
    "                ):\n",
    "                    # Stop if candidates are found or looped through more than there are candidates or maximum time reached\n",
    "\n",
    "                    print(\"While loop iteration %d\" % it)\n",
    "\n",
    "                    if it != 0:  # Because indices are not there in the first iteration\n",
    "                        for index in indices:\n",
    "                            scores_candidates_to_expand2[index] = 2 * score_predicted\n",
    "\n",
    "                    # print(\n",
    "                    #     \"\\n scores_candidates_to_expand2 after loop\",\n",
    "                    #     scores_candidates_to_expand2,\n",
    "                    # )\n",
    "                    # print(\"\\n indices\", indices)\n",
    "\n",
    "                    # do elementwise subtraction between score_predicted and scores_candidates_to_expand2\n",
    "                    subtractionList = []\n",
    "                    for x, y in zip(score_predicted, scores_candidates_to_expand2):\n",
    "                        print(\"\\n x, y\", x - y)\n",
    "                        subtractionList.append(x - y)\n",
    "\n",
    "                    # Do element wise subtraction between the prediction score of the x_ref and every element of the scores_candidates_to_expand2\n",
    "                    index_combi_max = np.argmax(subtractionList)\n",
    "                    if self.revert == 0:\n",
    "                        index_combi_max = np.argmax(subtractionList)\n",
    "                    else:\n",
    "                        index_combi_max = np.argmin(subtractionList)\n",
    "                    # index_shap_max = np.argmax(shap_candidates_to_expand2)\n",
    "                    # index_shap_min = np.argmin(shap_candidates_to_expand2)\n",
    "                    # if self.revert == 0:\n",
    "                    #     index_combi_max = index_shap_max\n",
    "                    # else:\n",
    "                    #     index_combi_max = index_shap_min\n",
    "                    # print(\n",
    "                    #     \"subtrac max \",\n",
    "                    #     index_combi_max,\n",
    "                    #     \" index_shap_max \",\n",
    "                    #     index_shap_max,\n",
    "                    # )\n",
    "                    # # Get the index of the maximum value -> Expand it\n",
    "                    # print(\n",
    "                    #     \"\\n index_combi_max\",\n",
    "                    #     candidates_to_expand_updated[np.argmax(subtractionList)],\n",
    "                    #     \"\\n index_shap_max\",\n",
    "                    #     candidates_to_expand_updated[index_combi_max],\n",
    "                    # )\n",
    "                    indices.append(index_combi_max)\n",
    "                    expanded_combis.append(\n",
    "                        candidates_to_expand_updated[index_combi_max]\n",
    "                    )\n",
    "                    # Add this combination to already expanded combinations as it will be expanded next by expand and prune function\n",
    "\n",
    "                    comb_to_expand = candidates_to_expand_updated[index_combi_max]\n",
    "                    # Expand the found combination with highest difference\n",
    "                    func = expand_and_prune(\n",
    "                        comb_to_expand,\n",
    "                        expanded_combis,\n",
    "                        feature_set,\n",
    "                        candidates_to_expand_updated,\n",
    "                        explanations_sets,\n",
    "                        scores_candidates_to_expand_updated,\n",
    "                        instance,\n",
    "                        self.classifier_fn,\n",
    "                        self.revert,\n",
    "                    )\n",
    "                    \"\"\"Returns:\n",
    "                        - explanation_candidates: combinations of features that are explanation\n",
    "                        candidates to be checked in the next iteration\n",
    "                        - candidates_to_expand: combinations of features that are candidates to\n",
    "                        expanded in next iterations or candidates for \"best-first\"\n",
    "                        - expanded_combis: [list] list of combinations of features that are already\n",
    "                        expanded as \"best-first\"\n",
    "                        - scores_candidates_to_expand: scores after perturbation for the candidate\n",
    "                        combinations of features to be expanded\n",
    "                        - scores_explanation_candidates: scores after perturbation of explanation candidates\"\"\"\n",
    "                    explanation_candidates = func[0]\n",
    "                    candidates_to_expand = func[1]\n",
    "                    expanded_combis = func[2]\n",
    "                    scores_candidates_to_expand = func[3]\n",
    "                    scores_explanation_candidates = func[4]\n",
    "\n",
    "                    it += 1\n",
    "\n",
    "                print(\n",
    "                    \"\\n\\n\\niteration - \", iteration, \" self.max_iter - \", self.max_iter\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(candidates_to_expand) - \",\n",
    "                    len(candidates_to_expand),\n",
    "                    \" != 0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\nlen(explanation_candidates) - \",\n",
    "                    len(explanation_candidates),\n",
    "                    \" !=0 \",\n",
    "                )\n",
    "                print(\n",
    "                    \"\\n\\n(time.time() - tic) - \",\n",
    "                    (time.time() - tic),\n",
    "                    \" self.time_maximum - \",\n",
    "                    self.time_maximum,\n",
    "                )\n",
    "            print(\"\\n Elapsed time %d \\n\" % (time.time() - tic))\n",
    "\n",
    "        # *** FINAL PART OF ALGORITHM ***\n",
    "        print(\"Iterations are done.\")\n",
    "\n",
    "        explanation_set = []\n",
    "        explanation_feature_names = []\n",
    "        for i in range(len(explanations)):\n",
    "            explanation_feature_names = []\n",
    "            for features in explanations[i]:\n",
    "                explanation_feature_names.append(self.feature_names[features])\n",
    "            explanation_set.append(explanation_feature_names)\n",
    "\n",
    "        if len(explanations) != 0:\n",
    "            lengths_explanation = []\n",
    "            for explanation in explanations:\n",
    "                l = len(explanation)\n",
    "                lengths_explanation.append(l)\n",
    "            minimum_size_explanation = np.min(lengths_explanation)\n",
    "\n",
    "        number_explanations = len(explanations)\n",
    "        if np.size(explanations_score_change) > 1:\n",
    "            inds = np.argsort(explanations_score_change, axis=0)\n",
    "            inds = np.fliplr([inds])[0]\n",
    "            inds_2 = []\n",
    "            for i in range(np.size(inds)):\n",
    "                inds_2.append(inds[i][0])\n",
    "            explanation_set_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanation_set_adjusted.append(explanation_set[j])\n",
    "            explanations_score_change_adjusted = []\n",
    "            for i in range(np.size(inds)):\n",
    "                j = inds_2[i]\n",
    "                explanations_score_change_adjusted.append(explanations_score_change[j])\n",
    "            explanation_set = explanation_set_adjusted\n",
    "            explanations_score_change = explanations_score_change_adjusted\n",
    "\n",
    "        time_elapsed = time.time() - tic\n",
    "        print(\"\\n Total elapsed time %d \\n\" % time_elapsed)\n",
    "\n",
    "        print(\n",
    "            \"If we remove the words \",\n",
    "            explanation_set[0 : self.max_explained],\n",
    "            \"From the review, the prediction will be reversed\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"explanation set\": explanation_set[0 : self.max_explained],\n",
    "            \"number active elements\": number_active_elements,\n",
    "            \"number explanations found\": number_explanations,\n",
    "            \"size smallest explanation\": minimum_size_explanation,\n",
    "            \"time elapsed\": time_elapsed,\n",
    "            \"differences score\": explanations_score_change[0 : self.max_explained],\n",
    "            \"iterations\": iteration,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47081014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:22.859676Z",
     "iopub.status.busy": "2023-05-23T22:24:22.858557Z",
     "iopub.status.idle": "2023-05-23T22:24:22.865677Z",
     "shell.execute_reply": "2023-05-23T22:24:22.864477Z"
    },
    "papermill": {
     "duration": 0.030868,
     "end_time": "2023-05-23T22:24:22.870299",
     "exception": false,
     "start_time": "2023-05-23T22:24:22.839431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classifier_fn_rf(x, negative_to_positive=0):\n",
    "    \"\"\"Returns the prediction probability of class 1 -> Not class 0\"\"\"\n",
    "    #print('loaded_plain_model_svc.decision_function(x) - ', loaded_plain_model_svc.decision_function(x))\n",
    "    prediction = loaded_plain_model_lr.predict_proba(x)\n",
    "    # If prediction is [1] retrurn the probability of class 1 else return probability of class 0\n",
    "    if (negative_to_positive == 1):\n",
    "        return prediction[:,0]\n",
    "    return prediction[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e52259f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:14.775696Z",
     "iopub.status.busy": "2023-05-23T22:24:14.775084Z",
     "iopub.status.idle": "2023-05-23T22:24:22.763555Z",
     "shell.execute_reply": "2023-05-23T22:24:22.762523Z"
    },
    "papermill": {
     "duration": 8.009867,
     "end_time": "2023-05-23T22:24:22.767023",
     "exception": false,
     "start_time": "2023-05-23T22:24:14.757156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47500000000002274\n",
      "The accuracy of the model on the test data is 0.836000\n"
     ]
    }
   ],
   "source": [
    "# Get threshold_classifier_probs\n",
    "p = np.sum(y_train_imdb)/np.size(y_train_imdb)\n",
    "\n",
    "probs = loaded_plain_model_rf.best_estimator_.predict(x_test_imdb)\n",
    "threshold_classifier_probs = np.percentile(probs,(52.5))\n",
    "print(threshold_classifier_probs)\n",
    "predictions_probs = (probs >= threshold_classifier_probs) \n",
    "\n",
    "accuracy_test = accuracy_score(y_test_imdb, np.array(predictions_probs))\n",
    "print(\"The accuracy of the model on the test data is %f\" %accuracy_test)\n",
    "\n",
    "#indices_probs_pos = np.nonzero(predictions_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9687a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60786524])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_fn_rf(x_test_imdb[10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba08552c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:22.905668Z",
     "iopub.status.busy": "2023-05-23T22:24:22.903844Z",
     "iopub.status.idle": "2023-05-23T22:24:22.911474Z",
     "shell.execute_reply": "2023-05-23T22:24:22.910280Z"
    },
    "papermill": {
     "duration": 0.028079,
     "end_time": "2023-05-23T22:24:22.914460",
     "exception": false,
     "start_time": "2023-05-23T22:24:22.886381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start initialization...\n",
      "score_predicted   [0.60786524]   initial_class   [1]\n",
      "candidates_to_expand  [OrderedSet([279]), OrderedSet([587]), OrderedSet([595]), OrderedSet([2118]), OrderedSet([2175]), OrderedSet([4009]), OrderedSet([4817]), OrderedSet([5584]), OrderedSet([6556]), OrderedSet([7500]), OrderedSet([7959]), OrderedSet([8016]), OrderedSet([8996]), OrderedSet([10208]), OrderedSet([10232]), OrderedSet([10648]), OrderedSet([11083]), OrderedSet([11241]), OrderedSet([11914]), OrderedSet([12805]), OrderedSet([13153]), OrderedSet([13234]), OrderedSet([13333]), OrderedSet([13650]), OrderedSet([13857]), OrderedSet([13876]), OrderedSet([14766]), OrderedSet([15223]), OrderedSet([15494]), OrderedSet([15666]), OrderedSet([17321]), OrderedSet([17960]), OrderedSet([19568]), OrderedSet([20643]), OrderedSet([22369]), OrderedSet([22522]), OrderedSet([24671]), OrderedSet([25250]), OrderedSet([25942]), OrderedSet([26254])]\n",
      "explanation_candidates  [OrderedSet([279]), OrderedSet([587]), OrderedSet([595]), OrderedSet([2118]), OrderedSet([2175]), OrderedSet([4009]), OrderedSet([4817]), OrderedSet([5584]), OrderedSet([6556]), OrderedSet([7500]), OrderedSet([7959]), OrderedSet([8016]), OrderedSet([8996]), OrderedSet([10208]), OrderedSet([10232]), OrderedSet([10648]), OrderedSet([11083]), OrderedSet([11241]), OrderedSet([11914]), OrderedSet([12805]), OrderedSet([13153]), OrderedSet([13234]), OrderedSet([13333]), OrderedSet([13650]), OrderedSet([13857]), OrderedSet([13876]), OrderedSet([14766]), OrderedSet([15223]), OrderedSet([15494]), OrderedSet([15666]), OrderedSet([17321]), OrderedSet([17960]), OrderedSet([19568]), OrderedSet([20643]), OrderedSet([22369]), OrderedSet([22522]), OrderedSet([24671]), OrderedSet([25250]), OrderedSet([25942]), OrderedSet([26254])]\n",
      "Initialization is complete.\n",
      "\n",
      " Elapsed time 0 \n",
      "\n",
      "\n",
      " Iteration 1 \n",
      "\n",
      "Run in first iteration -> perturbation done \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacements \n",
      " [OrderedSet(), OrderedSet([19394]), OrderedSet([11979]), OrderedSet(), OrderedSet([15375]), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet([9621]), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet([2077]), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet([13773]), OrderedSet(), OrderedSet(), OrderedSet([9346]), OrderedSet([5977]), OrderedSet([6876]), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet([15381]), OrderedSet([2089]), OrderedSet()] \n",
      "\n",
      "explanation_candidates \n",
      " [OrderedSet([279]), OrderedSet([587]), OrderedSet([595]), OrderedSet([2118]), OrderedSet([2175]), OrderedSet([4009]), OrderedSet([4817]), OrderedSet([5584]), OrderedSet([6556]), OrderedSet([7500]), OrderedSet([7959]), OrderedSet([8016]), OrderedSet([8996]), OrderedSet([10208]), OrderedSet([10232]), OrderedSet([10648]), OrderedSet([11083]), OrderedSet([11241]), OrderedSet([11914]), OrderedSet([12805]), OrderedSet([13153]), OrderedSet([13234]), OrderedSet([13333]), OrderedSet([13650]), OrderedSet([13857]), OrderedSet([13876]), OrderedSet([14766]), OrderedSet([15223]), OrderedSet([15494]), OrderedSet([15666]), OrderedSet([17321]), OrderedSet([17960]), OrderedSet([19568]), OrderedSet([20643]), OrderedSet([22369]), OrderedSet([22522]), OrderedSet([24671]), OrderedSet([25250]), OrderedSet([25942]), OrderedSet([26254])] \n",
      "\n",
      "replaced_instances \n",
      " [<1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 40 stored elements in List of Lists format>, <1x26698 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 39 stored elements in List of Lists format>] \n",
      "\n",
      "scores_explanation_candidates \n",
      " [array([0.62080496]), array([0.56612015]), array([0.60778996]), array([0.60172373]), array([0.57681526]), array([0.60305325]), array([0.62094737]), array([0.58817361]), array([0.62174474]), array([0.64133546]), array([0.68402017]), array([0.72779579]), array([0.60065358]), array([0.62123425]), array([0.43747013]), array([0.63261082]), array([0.57729178]), array([0.60675781]), array([0.59121573]), array([0.61613335]), array([0.51009165]), array([0.66714146]), array([0.58820327]), array([0.61237093]), array([0.62515819]), array([0.49938268]), array([0.61407248]), array([0.60877833]), array([0.61335262]), array([0.61079873]), array([0.60512659]), array([0.66671249]), array([0.56278149]), array([0.62509308]), array([0.60340173]), array([0.60655367]), array([0.6000797]), array([0.58692141]), array([0.49849045]), array([0.59229704])] \n",
      "\n",
      "\n",
      "-------------Max length updated to -  1\n",
      "\n",
      "len(candidates_to_expand_updated) 0  0 \n",
      "\n",
      "nb_explanations 1  >= self.max_explained  1\n",
      "nb_explanations Stop iterations...\n",
      "scores_candidates_to_expand_updated   []\n",
      "\n",
      " Elapsed time 3 \n",
      "\n",
      "Iterations are done.\n",
      "argmin OrderedSet([10232])\n",
      "feature_replacement [2077]\n",
      "new_replacements [[2077]]\n",
      "replacementfeature ['bad']\n",
      "replacementWords [{'feature': 'good', 'replacement': 'bad'}]\n",
      "final_prob [0.43747013]\n",
      "\n",
      " Total elapsed time 4 \n",
      "\n",
      "If we change the words  [['good']] From the review, the prediction will be reversed\n",
      "indices_active_elements [  279   587   595  2118  2175  4009  4817  5584  6556  7500  7959  8016\n",
      "  8996 10208 10232 10648 11083 11241 11914 12805 13153 13234 13333 13650\n",
      " 13857 13876 14766 15223 15494 15666 17321 17960 19568 20643 22369 22522\n",
      " 24671 25250 25942 26254]\n"
     ]
    }
   ],
   "source": [
    "explainer_shap = SEDC_Explainer(feature_names = feature_names,\n",
    "                          threshold_classifier = threshold_classifier_probs,\n",
    "                          classifier_fn = classifier_fn_rf,\n",
    "                          max_iter = 50,\n",
    "                          time_maximum = 120)\n",
    "\n",
    "explanation_normal = explainer_shap.explanation(x_test_imdb[10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2082d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_no_shap = SEDC_Explainer_no(feature_names = feature_names,\n",
    "                          threshold_classifier = threshold_classifier_probs,\n",
    "                          classifier_fn = classifier_fn_rf,\n",
    "                          max_iter = 50,\n",
    "                          time_maximum = 120)\n",
    "explanation_no_shap = explainer_no_shap.explanation(x_test_imdb[4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "541af84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-23T22:24:58.903375Z",
     "iopub.status.busy": "2023-05-23T22:24:58.902856Z",
     "iopub.status.idle": "2023-05-23T22:24:58.913086Z",
     "shell.execute_reply": "2023-05-23T22:24:58.911659Z"
    },
    "papermill": {
     "duration": 0.042328,
     "end_time": "2023-05-23T22:24:58.915972",
     "exception": false,
     "start_time": "2023-05-23T22:24:58.873644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation set': [['like']],\n",
       " 'number active elements': 65,\n",
       " 'number explanations found': 2,\n",
       " 'size smallest explanation': 1,\n",
       " 'time elapsed': 1.142374038696289,\n",
       " 'differences score': [array([0.51314498])],\n",
       " 'iterations': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_no_shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 127.097894,
   "end_time": "2023-05-23T22:25:02.784322",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-23T22:22:55.686428",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
