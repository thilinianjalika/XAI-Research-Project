{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "9bVgiFiQYiSS"
            },
            "source": [
                "# Initialisation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "cellView": "form",
                "executionInfo": {
                    "elapsed": 3329,
                    "status": "ok",
                    "timestamp": 1686739620332,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "P9uxhgMQYiSX",
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# @title Install Packages\n",
                "\n",
                "!pip install -qq ordered_set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "cellView": "form",
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 3043,
                    "status": "ok",
                    "timestamp": 1686739623368,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "flFwmgm8Y14i",
                "outputId": "8b5eb0fb-967d-4784-ba45-85f2b06fb919"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
                    ]
                }
            ],
            "source": [
                "# @title Mount Google Drive for Credentials\n",
                "\n",
                "from google.colab import drive\n",
                "drive.mount(\"/content/drive\")\n",
                "!rm -r -f /content/sample_data\n",
                "!cp -r /content/drive/MyDrive/.kaggle ~"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 239763,
                    "status": "ok",
                    "timestamp": 1686739863120,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "5OexGopuYtfI",
                "outputId": "7a083161-a021-4028-f7f9-bcc65476bea9"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
                        "\r  0% 0.00/25.7M [00:00<?, ?B/s]\r 51% 13.0M/25.7M [00:00<00:00, 134MB/s]\n",
                        "\r100% 25.7M/25.7M [00:00<00:00, 176MB/s]\n",
                        "mkdir: cannot create directory ‘/content/data’: File exists\n",
                        "replace /content/data/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Downloading models.zip to /content\n",
                        " 99% 16.0M/16.2M [00:00<00:00, 167MB/s]\n",
                        "100% 16.2M/16.2M [00:00<00:00, 168MB/s]\n",
                        "mkdir: cannot create directory ‘/content/models-weights’: File exists\n",
                        "replace /content/models-weights/grid_imdb_knn.pickle? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
                    ]
                }
            ],
            "source": [
                "# @title Downloads\n",
                "\n",
                "# nltk\n",
                "import nltk\n",
                "nltk.download('wordnet', quiet=True)\n",
                "nltk.download('stopwords', quiet=True)\n",
                "nltk.download('punkt', quiet=True)\n",
                "\n",
                "# imdb sentiment dataset\n",
                "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
                "!mkdir /content/data\n",
                "!mv ./imdb-dataset-of-50k-movie-reviews.zip /content/data/imdb-dataset-of-50k-movie-reviews.zip\n",
                "!unzip -qq /content/data/imdb-dataset-of-50k-movie-reviews.zip -d /content/data/imdb-dataset-of-50k-movie-reviews\n",
                "\n",
                "# model weights\n",
                "!kaggle datasets download -d tharushalekamge/models\n",
                "!mkdir /content/models-weights\n",
                "!mv ./models.zip /content/models-weights/models.zip\n",
                "!unzip -qq /content/models-weights/models.zip -d /content/models-weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {
                "cellView": "form",
                "executionInfo": {
                    "elapsed": 22,
                    "status": "ok",
                    "timestamp": 1686739863122,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "FktEw3aCPeNg"
            },
            "outputs": [],
            "source": [
                "# @title Static paths\n",
                "\n",
                "dataset_csv_path = \"/content/data/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
                "model_weights_dir = \"/content/models-weights\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "7r-WrRBrOYJU"
            },
            "source": [
                "# Create dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "cellView": "form",
                "executionInfo": {
                    "elapsed": 20,
                    "status": "ok",
                    "timestamp": 1686739863124,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "d0tuyM0-YiSa",
                "trusted": true
            },
            "outputs": [],
            "source": [
                "# @title Module Imports\n",
                "\n",
                "import time\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.metrics import roc_auc_score, accuracy_score\n",
                "from sklearn.model_selection import ParameterGrid\n",
                "from sklearn.svm import SVC\n",
                "import sklearn.feature_extraction\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.feature_extraction.text import TfidfTransformer\n",
                "\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "\n",
                "from bs4 import BeautifulSoup\n",
                "import re\n",
                "import pickle\n",
                "import seaborn as sns\n",
                "\n",
                "from ordered_set import OrderedSet\n",
                "from scipy.sparse import lil_matrix\n",
                "from itertools import compress"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {
                "executionInfo": {
                    "elapsed": 17,
                    "status": "ok",
                    "timestamp": 1686739863125,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "peZNRrqQoCOK"
            },
            "outputs": [],
            "source": [
                "# @title Dataset definition\n",
                "\n",
                "class IMDBDataset:\n",
                "  def _strip_html(self, text):\n",
                "    soup = BeautifulSoup(text, \"html.parser\")\n",
                "    return soup.get_text()\n",
                "\n",
                "  def _remove_special_characters(self, text, remove_digits=True):\n",
                "      pattern=r'[^a-zA-z0-9\\s]'\n",
                "      text=re.sub(pattern,'',text)\n",
                "      return text\n",
                "\n",
                "  def _remove_stopwords(self, text, is_lower_case=False):\n",
                "      tokens = self.tokenizer.tokenize(text)\n",
                "      tokens = [token.strip() for token in tokens]\n",
                "      if is_lower_case:\n",
                "          filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
                "      else:\n",
                "          filtered_tokens = [token for token in tokens if token.lower() not in self.stop_words]\n",
                "      filtered_text = ' '.join(filtered_tokens)\n",
                "      return filtered_text\n",
                "\n",
                "  def _lemmatize_text(self, text):\n",
                "      words=word_tokenize(text)\n",
                "      edited_text = ''\n",
                "      for word in words:\n",
                "          lemma_word=self.lemmatizer.lemmatize(word)\n",
                "          extra=\" \"+str(lemma_word)\n",
                "          edited_text+=extra\n",
                "      return edited_text\n",
                "\n",
                "  def __init__(self, stop_words, tokenizer, lemmatizer, loaded_vectorizer, label_binarizer, dataset_csv_path):\n",
                "    self.stop_words = stop_words\n",
                "    self.tokenizer = tokenizer\n",
                "    self.lemmatizer = lemmatizer\n",
                "\n",
                "    ## Import\n",
                "    data = pd.read_csv(dataset_csv_path)\n",
                "    data = data.sample(10000)\n",
                "\n",
                "    ## Preprocess\n",
                "    data.review = data.review.str.lower()\n",
                "    data.review = data.review.apply(self._strip_html)\n",
                "    data.review = data.review.apply(self._remove_special_characters)\n",
                "    data.review = data.review.apply(self._remove_stopwords)\n",
                "    data.review = data.review.apply(self._lemmatize_text)\n",
                "\n",
                "    ## Split Data\n",
                "    x_imdb = data['review']\n",
                "    y_imdb = data['sentiment']\n",
                "\n",
                "    x_train_i, x_test_i, y_train_i, y_test_i = train_test_split(x_imdb,y_imdb,test_size=0.2)\n",
                "    x_test, x_val, y_test_i, y_val_i = train_test_split(x_test_i,y_test_i,test_size=0.5)\n",
                "\n",
                "    ## X data\n",
                "    x_train_imdb = loaded_vectorizer.fit_transform(x_train_i)\n",
                "    x_test_imdb = loaded_vectorizer.transform(x_test)\n",
                "    x_val_imdb = loaded_vectorizer.transform(x_val)\n",
                "\n",
                "    # Y data - Positive is 1\n",
                "    y_train_imdb = label_binarizer.fit_transform(y_train_i)\n",
                "    y_test_imdb = label_binarizer.fit_transform(y_test_i)\n",
                "    y_val_imdb = label_binarizer.fit_transform(y_val_i)\n",
                "\n",
                "    self.x_train_imdb = x_train_imdb\n",
                "    self.x_test_imdb = x_test_imdb\n",
                "    self.x_val_imdb = x_val_imdb\n",
                "    self.y_train_imdb = y_train_imdb\n",
                "    self.y_test_imdb = y_test_imdb\n",
                "    self.y_val_imdb = y_val_imdb\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 13663,
                    "status": "ok",
                    "timestamp": 1686739876774,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "6_ZrsD-hMj9-",
                "outputId": "6c825b8e-0fb1-45e3-c408-30d370bb0778"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<ipython-input-36-e6a152c6ae4c>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
                        "  soup = BeautifulSoup(text, \"html.parser\")\n"
                    ]
                }
            ],
            "source": [
                "# @title Dataset instantiation\n",
                "\n",
                "loaded_vocab = pickle.load(open(f'{model_weights_dir}/vectorizer_imdb.pkl', 'rb'))\n",
                "stop_words = set(stopwords.words('english'))\n",
                "tokenizer = nltk.tokenize.toktok.ToktokTokenizer()\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "loaded_vectorizer = TfidfVectorizer(min_df=2, vocabulary=loaded_vocab)\n",
                "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
                "feature_names = loaded_vectorizer.get_feature_names_out()\n",
                "\n",
                "ds = IMDBDataset(stop_words, tokenizer, lemmatizer, loaded_vectorizer, label_binarizer, dataset_csv_path)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "5k-p3Hu3Ptw1"
            },
            "source": [
                "# Training"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "up2MXZ7gYiSi"
            },
            "source": [
                "## Train RF model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2023-05-24T01:59:14.021625Z",
                    "iopub.status.idle": "2023-05-24T01:59:14.022487Z",
                    "shell.execute_reply": "2023-05-24T01:59:14.022216Z",
                    "shell.execute_reply.started": "2023-05-24T01:59:14.022189Z"
                },
                "id": "KD9f8npnYiSi"
            },
            "outputs": [],
            "source": [
                "# Number of trees in random forest\n",
                "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
                "# Maximum number of levels in tree\n",
                "max_depth = [int(x) for x in np.linspace(10, 100, num = 5)]\n",
                "max_depth.append(None)\n",
                "# Minimum number of samples required to split a node\n",
                "min_samples_split = [2, 5, 10]\n",
                "# Minimum number of samples required at each leaf node\n",
                "min_samples_leaf = [1, 2, 4]\n",
                "# Method of selecting samples for training each tree\n",
                "bootstrap = [True, False]\n",
                "# Create the grid\n",
                "grid_rf = {'n_estimators': n_estimators,\n",
                "               'max_depth': max_depth,\n",
                "               'min_samples_split': min_samples_split,\n",
                "               'min_samples_leaf': min_samples_leaf,\n",
                "               'bootstrap': bootstrap}\n",
                "print(grid_rf)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2023-05-24T01:59:14.024015Z",
                    "iopub.status.idle": "2023-05-24T01:59:14.024474Z",
                    "shell.execute_reply": "2023-05-24T01:59:14.024286Z",
                    "shell.execute_reply.started": "2023-05-24T01:59:14.024258Z"
                },
                "id": "tEzjMAlVYiSj"
            },
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.status.busy": "2023-05-24T01:59:14.025697Z",
                    "iopub.status.idle": "2023-05-24T01:59:14.026125Z",
                    "shell.execute_reply": "2023-05-24T01:59:14.025929Z",
                    "shell.execute_reply.started": "2023-05-24T01:59:14.025909Z"
                },
                "id": "QZILUtoKYiSj"
            },
            "outputs": [],
            "source": [
                "grid_imdb_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions = grid_rf, n_iter = 200, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
                "# # Fit the random search model\n",
                "grid_imdb_rf.fit(ds.x_train_imdb, ds.y_train_imdb.ravel())\n",
                "pickle.dump(grid_imdb_rf, open('grid_imdb_rf.pickle', \"wb\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "NuKAyto2YiSk"
            },
            "source": [
                "## Train SVC Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 131,
            "metadata": {
                "executionInfo": {
                    "elapsed": 421,
                    "status": "ok",
                    "timestamp": 1686745062320,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "oVEo4umEYiSk"
            },
            "outputs": [],
            "source": [
                "# Param Optimisation\n",
                "param_grid_imdb = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf']}\n",
                "grid_imdb_svc = GridSearchCV(SVC(probability=True),param_grid_imdb,refit=True,verbose=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "myKK_fX8YiSk",
                "outputId": "f9a0f90b-05e3-4295-f7c7-dcc327ebf7ed"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
                    ]
                }
            ],
            "source": [
                "grid_imdb_svc.fit(ds.x_train_imdb,ds.y_train_imdb.ravel())\n",
                "pickle.dump(grid_imdb_svc, open('grid_imdb_svc.pickle', \"wb\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "SY3pUI7EYiSl"
            },
            "source": [
                "## Train KNN model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "W1wrjBhuYiSl"
            },
            "outputs": [],
            "source": [
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "grid_params_imdb_knn = { 'n_neighbors' : [30,40,50,60,70,80,90], 'metric' : ['manhattan', 'minkowski'], 'weights': ['uniform', 'distance']}\n",
                "grid_imdb_knn = GridSearchCV(KNeighborsClassifier(), grid_params_imdb_knn, n_jobs=-1,verbose=2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "5aZsJHx9YiSl"
            },
            "outputs": [],
            "source": [
                "grid_imdb_knn.fit(ds.x_train_imdb,np.ravel(ds.y_train_imdb,order='C'))\n",
                "pickle.dump(grid_imdb_knn, open('grid_imdb_knn.pickle', \"wb\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "JiEMj2yVYiSm"
            },
            "source": [
                "## Train LR Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "P-pJK2fpYiSm"
            },
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "param_grid_imdb_lr = [\n",
                "    {'penalty' : ['l1', 'l2', 'elasticnet'],\n",
                "    'C' : np.logspace(-4, 4, 20),\n",
                "    'solver' : ['lbfgs','newton-cg','sag'],\n",
                "    'max_iter' : [100, 1000, 5000]\n",
                "    }\n",
                "]\n",
                "grid_imdb_lr = GridSearchCV(LogisticRegression(), param_grid = param_grid_imdb_lr, cv = 3, verbose=2, n_jobs=-1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "3wb9Q7sKYiSm"
            },
            "outputs": [],
            "source": [
                "grid_imdb_lr.fit(ds.x_train_imdb, np.ravel(ds.y_train_imdb,order='C'))\n",
                "pickle.dump(grid_imdb_lr, open('grid_imdb_lr.pickle', \"wb\"))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "INf0tUGIYiSm"
            },
            "source": [
                "# Load Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 11,
                    "status": "ok",
                    "timestamp": 1686739876774,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "mg2it509YiSn",
                "outputId": "823b8151-9b56-4240-fafc-330b6a10f761",
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<ipython-input-38-83c76ea3603f>:2: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
                        "  loaded_svc_imdb = pickle.load(open(f'{model_weights_dir}/grid_imdb_svc.pickle', \"rb\"))\n",
                        "<ipython-input-38-83c76ea3603f>:3: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
                        "  loaded_knn_imdb = pickle.load(open(f'{model_weights_dir}/grid_imdb_knn.pickle', \"rb\"))\n"
                    ]
                }
            ],
            "source": [
                "# Load\n",
                "loaded_svc_imdb = pickle.load(open(f'{model_weights_dir}/grid_imdb_svc.pickle', \"rb\"))\n",
                "loaded_lr_imdb = pickle.load(open(f'{model_weights_dir}/grid_imdb_lr.pickle', \"rb\"))\n",
                "loaded_rf_imdb = pickle.load(open(f'{model_weights_dir}/grid_imdb_rf.pickle', \"rb\"))\n",
                "loaded_knn_imdb = pickle.load(open(f'{model_weights_dir}/grid_imdb_knn.pickle', \"rb\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 7,
                    "status": "ok",
                    "timestamp": 1686739876775,
                    "user": {
                        "displayName": "Avishka Perera",
                        "userId": "05205841493968506808"
                    },
                    "user_tz": -330
                },
                "id": "rLnVkjyFYiSn",
                "outputId": "9fa322cd-9956-40d3-c2e3-66f2dc05ec43",
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
                        "{'C': 4.281332398719396, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
                        "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': None, 'bootstrap': False}\n",
                        "{'metric': 'minkowski', 'n_neighbors': 90, 'weights': 'distance'}\n"
                    ]
                }
            ],
            "source": [
                "print(loaded_svc_imdb.best_params_)\n",
                "print(loaded_lr_imdb.best_params_)\n",
                "print(loaded_rf_imdb.best_params_)\n",
                "print(loaded_knn_imdb.best_params_)"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "collapsed_sections": [
                "up2MXZ7gYiSi",
                "NuKAyto2YiSk",
                "JiEMj2yVYiSm"
            ],
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
